{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Import all required modules, hook PySyft, declare functions for simulating FL environment (Star Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n",
    "\n",
    "####################\n",
    "# Required Modules #\n",
    "####################\n",
    "\n",
    "# Generic\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Libs\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import sklearn.datasets as skld\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from tqdm.notebook import trange\n",
    "from IPython.display import display\n",
    "\n",
    "##################\n",
    "# Configurations #\n",
    "##################\n",
    "\n",
    "# Integrating PyTorch with PySyft\n",
    "pt_hook = sy.TorchHook(th)\n",
    "\n",
    "########################\n",
    "# Simulation Functions #\n",
    "########################\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    \"\"\" Simulates the existence of N workers\n",
    "    \n",
    "    Args:\n",
    "        n_workers (int): No. of virtual workers to simulate\n",
    "    Returns:\n",
    "        N virtual workers (list(sy.VirtualWorker))\n",
    "    \"\"\"\n",
    "    return [\n",
    "        sy.VirtualWorker(\n",
    "            pt_hook, id=f\"worker{i+1}\"\n",
    "        ).clear_objects(\n",
    "        ) for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "def connect_to_crypto_provider():\n",
    "    \"\"\" Simulates the existence of an arbitor to facilitate\n",
    "        model generation & client-side utilisation\n",
    "        \n",
    "    Returns:\n",
    "        Arbiter (i.e. TTP) (sy.VirtualWorker)\n",
    "    \"\"\"\n",
    "    return sy.VirtualWorker(\n",
    "        pt_hook, \n",
    "        id=\"crypto_provider\"\n",
    "    ).clear_objects()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# PyTorch Model Class Declaration\n",
    "\n",
    "Declare the model classes used for simulation, along with requisite arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    The Model class declares a PySyft neural network based on the specifications contained\n",
    "    inside the structural_definition dictionary. \n",
    "\n",
    "    Args:\n",
    "        structural_definition (Dict): For each layer of the network, specify name, input\n",
    "                                      and output size, activation function, layer_type. \n",
    "\n",
    "    Attributes:\n",
    "        layers (list): This is a list of tuples, each containing the layer name and \n",
    "                       activation function. \n",
    "    \"\"\"\n",
    "    def __init__(self, structural_definition):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        for layer in structural_definition:\n",
    "            layer_params = structural_definition[layer]\n",
    "            layer_type = self.__parse_layer_type(layer_params['layer_type'])\n",
    "            layer_size_mapping = layer_params['layer_size_mapping']\n",
    "            activation = self.__parse_activation_type(layer_params['activation'])\n",
    "            \n",
    "            setattr(self, \n",
    "                    layer, \n",
    "                    layer_type(**layer_size_mapping))\n",
    "            \n",
    "            self.layers.append((layer, activation))\n",
    "            \n",
    "    ###########\n",
    "    # Helpers #\n",
    "    ###########\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_layer_type(layer_type):\n",
    "        \"\"\" Detects layer type of a specified layer from configuration\n",
    "\n",
    "        Args:\n",
    "            layer_type (str): Layer type to initialise\n",
    "        Returns:\n",
    "            Layer definition (Function)\n",
    "        \"\"\"\n",
    "        if layer_type == \"linear\":\n",
    "            return nn.Linear\n",
    "        elif layer_type == 'conv2d':\n",
    "            return nn.Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Specified layer type is currently not supported!\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_activation_type(activation_type):\n",
    "        \"\"\" Detects activation function specified from configuration\n",
    "\n",
    "        Args:\n",
    "            activation_type (str): Activation function to use\n",
    "        Returns:\n",
    "            Activation definition (Function)\n",
    "        \"\"\"\n",
    "        if activation_type == \"sigmoid\":\n",
    "            return th.sigmoid\n",
    "        elif activation_type == \"relu\":\n",
    "            return th.relu\n",
    "        elif activation_type == \"nil\":\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(\"Specified activation is currently not supported!\")\n",
    "\n",
    "    ##################\n",
    "    # Core Functions #\n",
    "    ##################\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_activation_tuple in self.layers:\n",
    "            current_layer =  getattr(self, layer_activation_tuple[0])\n",
    "            if layer_activation_tuple[1] is None:\n",
    "                x = current_layer(x)\n",
    "            else:\n",
    "                x = layer_activation_tuple[1](current_layer(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Specific Set up\n",
    "\n",
    "Set up FL environments, where workers and a ttp are initialized, and datasets are deployed to individual workers. Computation over these datasets should be done using pointers, for truer-to-life FL simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secret_share(tensor, workers, crypto_provider, precision_fractional):\n",
    "    \"\"\" Transform to fixed precision and secret share a tensor \n",
    "    \n",
    "    Args:\n",
    "        tensor             (PointerTensor): Pointer to be shared\n",
    "        workers   (list(sy.VirtualWorker)): Involved workers of the grid\n",
    "        crypto_provider (sy.VirtualWorker): Arbiter (i.e. TTP) of the grid\n",
    "        precision_fractional (int): Precision for casting integer ring arithimetic\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tensor\n",
    "        .fix_precision(precision_fractional=precision_fractional)\n",
    "        .share(\n",
    "            *workers, \n",
    "            crypto_provider=crypto_provider, \n",
    "            requires_grad=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "def setup_FL_env(training_datasets, validation_datasets, \n",
    "                 testing_dataset, is_shared=False):\n",
    "    \"\"\" Sets up a basic federated learning environment using virtual workers,\n",
    "        with a allocated arbiter (i.e. TTP) to faciliate in model development\n",
    "        & utilisation, and deploys datasets to their respective workers\n",
    "        \n",
    "    Args:\n",
    "\n",
    "        training_datasets   (dict(tuple(th.Tensor))): Datasets to be used for training\n",
    "        validation_datasets (dict(tuple(th.Tensor))): Datasets to be used for validation\n",
    "        testing_dataset           (tuple(th.Tensor)): Datasets to be used for testing\n",
    "        is_shared (bool): Toggles if SMPC encryption protocols are active\n",
    "    Returns:\n",
    "        training_pointers  (dict(sy.BaseDataset))\n",
    "        validation_pointer (dict(sy.BaseDataset))\n",
    "        testing_pointer    (sy.BaseDataset)\n",
    "        workers            (list(sy.VirtualWorker))\n",
    "        crypto_provider    (sy.VirtualWorker)\n",
    "    \"\"\"\n",
    "    # Simulate FL computation amongst K worker nodes, \n",
    "    # where K is the no. of datasets to be federated\n",
    "    workers = connect_to_workers(n_workers=len(training_datasets))\n",
    "    \n",
    "    # Allow for 1 exchanger/Arbiter (i.e. TTP)\n",
    "    crypto_provider = connect_to_crypto_provider()\n",
    "    crypto_provider.clear_objects()\n",
    "    \n",
    "    assert (len(crypto_provider._objects) == 0)\n",
    "    \n",
    "    # Send training & validation datasets to their respective workers\n",
    "    training_pointers = {}\n",
    "    validation_pointers = {}\n",
    "    for w_idx in range(len(workers)):\n",
    "\n",
    "        # Retrieve & prepare worker for receiving dataset\n",
    "        curr_worker = workers[w_idx]\n",
    "        curr_worker.clear_objects()\n",
    "\n",
    "        assert (len(curr_worker._objects) == 0)\n",
    "\n",
    "        train_data = training_datasets[w_idx]\n",
    "        validation_data = validation_datasets[w_idx]\n",
    "        \n",
    "        # Cast dataset into a Tensor & send it to the relevant worker\n",
    "        train_pointer = sy.BaseDataset(*train_data).send(curr_worker)\n",
    "        validation_pointer = sy.BaseDataset(*validation_data).send(curr_worker)\n",
    "        \n",
    "        # Store data pointers for subsequent reference\n",
    "        training_pointers[curr_worker] = train_pointer\n",
    "        validation_pointers[curr_worker] = validation_pointer\n",
    "    \n",
    "    # 'Me' serves as the client -> test pointer stays with me, but is shared via SMPC\n",
    "    testing_pointer = sy.BaseDataset(*testing_dataset).send(crypto_provider)\n",
    "    \n",
    "    return training_pointers, validation_pointers, testing_pointer, workers, crypto_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_FL_batches(model_hyperparams, train_pointers, validation_pointers, test_pointer): \n",
    "    \"\"\" Supplementary function to convert initialised datasets into their\n",
    "        SGD compatible dataloaders in the context of PySyft's federated learning\n",
    "        (NOTE: This is based on the assumption that querying database size does\n",
    "               not break FL abstraction (i.e. not willing to share quantity))\n",
    "    Args:\n",
    "        model_hyperparams                      (model_hyperparams): Parameters defining current experiment\n",
    "        train_pointers      (dict(sy.BaseDataset)): Distributed datasets for training\n",
    "        validation_pointers (dict(sy.BaseDataset)): Distributed datasets for model calibration\n",
    "        test_pointer              (sy.BaseDataset): Distributed dataset for verifying performance\n",
    "    Returns:\n",
    "        train_loaders     (sy.FederatedDataLoader)\n",
    "        validation_loader (sy.FederatedDataLoader)\n",
    "        test_loader       (sy.FederatedDataLoader)\n",
    "    \"\"\"\n",
    "    \n",
    "    def construct_FL_loader(data_pointer, **kwargs):\n",
    "        \"\"\" Cast paired data & labels into configured tensor dataloaders\n",
    "        Args:\n",
    "            dataset (list(sy.BaseDataset)): A tuple of X features & y labels\n",
    "            kwargs: Additional parameters to configure PyTorch's Dataloader\n",
    "        Returns:\n",
    "            Configured dataloader (th.utils.data.DataLoader)\n",
    "        \"\"\"\n",
    "        federated_dataset = sy.FederatedDataset(data_pointer)\n",
    "        \n",
    "#         print(federated_dataset)\n",
    "        \n",
    "        federated_data_loader = sy.FederatedDataLoader(\n",
    "            federated_dataset, \n",
    "            batch_size=(\n",
    "                model_hyperparams['batch_size']\n",
    "                if model_hyperparams['batch_size'] \n",
    "                else len(federated_dataset)\n",
    "            ), \n",
    "            shuffle=True,\n",
    "            iter_per_worker=True, # for subsequent parallelization\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return federated_data_loader\n",
    "        \n",
    "        \n",
    "    # Load training pointers into a configured federated dataloader\n",
    "    train_loader = construct_FL_loader(train_pointers.values())\n",
    "    \n",
    "    # Load validation pointer into a configured federated dataloader\n",
    "    validation_loader = construct_FL_loader(validation_pointers.values())\n",
    "    \n",
    "    # Load testing dataset into a configured federated dataloader\n",
    "    test_loader = construct_FL_loader([test_pointer])\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "\n",
    "Perform training in the FL style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_FL_training(model_hyperparams, \n",
    "                        model_structure,\n",
    "                        datasets, \n",
    "                        workers, \n",
    "                        crypto_provider,\n",
    "                        optimizer=th.optim.SGD):\n",
    "    \"\"\" \n",
    "    Simulates a PySyft federated learning cycle using PyTorch, in order\n",
    "    to prove that it can be done conceptually using the PyTorch interface\n",
    "        \n",
    "    Args:\n",
    "        model_hyperparams (model_hyperparams): \n",
    "                                            Parameters defining current experiment\n",
    "        datasets  (sy.FederatedDataLoader): \n",
    "                                        Distributed training datasets\n",
    "        workers   (list(sy.VirtualWorker)): \n",
    "                                        Workers involved in training\n",
    "        crypto_provider (sy.VirtualWorker): \n",
    "                                        Arbiter supervising training\n",
    "        model     (nn.Module): \n",
    "                            Current PyTorch model to train\n",
    "        optimizer (th.optim): \n",
    "                            Optimizer to use\n",
    "    Returns:\n",
    "        global_model (nn.Module) : The trained model \n",
    "        global_states (dict)\n",
    "                        {timestep: nn.Module}\n",
    "                        : The record of trained global models at each timestep.\n",
    "        client_states (dict)\n",
    "                        {timestep {worker_id: nn.Module}}\n",
    "                        : The record of trained models for each worker at each\n",
    "                          timestep. \n",
    "        scale_coeffs (dict)\n",
    "                        {worker_id: float}\n",
    "                        : A dictionary of the update weightings for each worker\n",
    "                          based on individual dataset size. \n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = model_hyperparams['criterion']\n",
    "\n",
    "    def perform_parallel_training(datasets, \n",
    "                                  models, \n",
    "                                  optimizers, \n",
    "                                  criterions, \n",
    "                                  epochs):\n",
    "        \"\"\" \n",
    "        Parallelizes training across each distributed dataset (i.e. simulated worker)\n",
    "        Parallelization here refers to the training of all distributed models per\n",
    "        epoch.\n",
    "        NOTE: Current approach does not have early stopping implemented\n",
    "            \n",
    "        Args:\n",
    "            datasets   (dict(th.utils.data.DataLoader)): \n",
    "                                                       Distributed training datasets\n",
    "            models     (list(nn.Module)): \n",
    "                                        Simulated local models (after distribution)\n",
    "            optimizers (list(th.optim)): \n",
    "                                       Simulated local optimizers (after distribution)\n",
    "            criterions (list(th.nn)):  \n",
    "                                    Simulated local objective function (after distribution)\n",
    "            epochs (int): \n",
    "                        No. of epochs to train each local model\n",
    "        Returns:\n",
    "            trained local models\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            for worker in datasets:\n",
    "#                 print(\"========================\")\n",
    "#                 print(worker)\n",
    "                for batch_idx, batch in enumerate(datasets[worker]):\n",
    "#                     print(batch_idx)\n",
    "                    data = batch[0]\n",
    "                    labels = batch[1]\n",
    "                    '''\n",
    "                    ========================\n",
    "                    Each worker trains its own model individually.\n",
    "                    ========================\n",
    "                    '''\n",
    "                    curr_model = models[worker]\n",
    "                    curr_optimizer = optimizers[worker]\n",
    "                    curr_criterion = criterions[worker]\n",
    "\n",
    "                    # Zero gradients to prevent accumulation                    \n",
    "                    curr_model.train()\n",
    "                    curr_optimizer.zero_grad()\n",
    "\n",
    "                    # Forward Propagation\n",
    "                    predictions = curr_model(data.float())\n",
    "#                     print(predictions.shape)\n",
    "#                     print(labels.shape)\n",
    "\n",
    "                    if model_hyperparams['is_condensed']:\n",
    "                        loss = curr_criterion(predictions, labels.float())\n",
    "                    else:\n",
    "                        loss = curr_criterion(predictions, labels.long())\n",
    "\n",
    "                    # Backward propagation\n",
    "                    loss.backward()\n",
    "                    curr_optimizer.step()\n",
    "\n",
    "                    # Update models, optimisers & losses\n",
    "                    models[worker] = curr_model\n",
    "                    optimizers[worker] = curr_optimizer\n",
    "                    criterions[worker] = curr_criterion\n",
    "\n",
    "                    assert (models[worker] == curr_model and \n",
    "                            optimizers[worker] == curr_optimizer and \n",
    "                            criterions[worker] == curr_criterion)\n",
    "\n",
    "        trained_models = {w: m.send(crypto_provider) for w,m in models.items()}\n",
    "\n",
    "        return trained_models\n",
    "    \n",
    "    def calculate_global_params(global_model, models, datasets):\n",
    "        \"\"\" Aggregates weights from trained locally trained models after a round.\n",
    "        \n",
    "        Args:\n",
    "            global_model   (nn.Module): Global model to be trained federatedly\n",
    "            models   (dict(nn.Module)): Simulated local models (after distribution)\n",
    "            datasets (dict(th.utils.data.DataLoader)): Distributed training datasets\n",
    "        Returns:\n",
    "            Aggregated parameters (OrderedDict)\n",
    "        \"\"\"\n",
    "        param_types = global_model.state_dict().keys()\n",
    "        model_states = {w: m.state_dict() for w,m in models.items()}\n",
    "        \n",
    "        # Calculate scaling factors for each worker\n",
    "        scale_coeffs = {w: 1/len(list(datasets.keys())) for w in list(datasets.keys())}\n",
    "\n",
    "        # PyTorch models can only swap weights of the same structure\n",
    "        # Hence, aggregate weights while maintaining original layering structure\n",
    "        aggregated_params = OrderedDict()\n",
    "        \n",
    "        '''\n",
    "        ======================\n",
    "        Grab the param_states\n",
    "        ======================\n",
    "        '''\n",
    "        params = {}\n",
    "        \n",
    "        for p_type in param_types:\n",
    "            #param_states = [th.mul(ms[p_type], sc) \n",
    "            #                for ms,sc in zip(model_states, scale_coeffs)]\n",
    "            param_states = [\n",
    "                th.mul(\n",
    "                    model_states[w][p_type],\n",
    "                    scale_coeffs[w]\n",
    "                ).get().get() for w in workers\n",
    "            ]\n",
    "            \n",
    "            '''\n",
    "            ======================\n",
    "            Grab the param_states\n",
    "            ======================\n",
    "            '''   \n",
    "            params.update({p_type : param_states})\n",
    "            \n",
    "            layer_shape = tuple(global_model.state_dict()[p_type].shape)\n",
    "            \n",
    "            '''\n",
    "            ======================\n",
    "            Modification made here to allow multiple layers.\n",
    "            ======================\n",
    "            '''  \n",
    "            aggregated_params[p_type] = th.zeros(param_states[0].shape, dtype=th.float64)\n",
    "            for param_state in param_states:\n",
    "                aggregated_params[p_type] += param_state\n",
    "            aggregated_params[p_type] = aggregated_params[p_type].view(*layer_shape)\n",
    "\n",
    "        return aggregated_params, params, scale_coeffs\n",
    "\n",
    "    # Generate a global model & send it to the TTP\n",
    "    \n",
    "    template_model = Model(model_structure)\n",
    "    \n",
    "    global_model = copy.deepcopy(template_model).send(crypto_provider)\n",
    "    \n",
    "    print(\"Global model parameters:\\n\", [p.location for p in list(global_model.parameters())],\n",
    "          \"\\nID:\\n\", [p.id_at_location for p in list(global_model.parameters())],\n",
    "          \"\\n Cloning effect on global model:\\n\", [p.clone() for p in list(global_model.parameters())])\n",
    "    \n",
    "    rounds = 0\n",
    "    pbar = tqdm(total=model_hyperparams['rounds'], desc='Rounds', leave=True)\n",
    "    \n",
    "    '''\n",
    "    * Dicts for model and client states\n",
    "    \n",
    "    '''\n",
    "    global_states = {}\n",
    "    client_states = {}\n",
    "    global_model_state_dicts = {}\n",
    "\n",
    "    client_template = copy.deepcopy(template_model)\n",
    "    \n",
    "    while rounds < model_hyperparams['rounds']:\n",
    "\n",
    "        local_models = {w: copy.deepcopy(client_template).send(w) for w in workers}\n",
    "\n",
    "        optimizers = {\n",
    "            w: optimizer(\n",
    "                params=model.parameters(), \n",
    "                lr=model_hyperparams['lr'], \n",
    "                weight_decay=model_hyperparams['decay']\n",
    "            ) for w, model in local_models.items()\n",
    "        }\n",
    "        \n",
    "        criterions = {w: criterion(reduction='mean') \n",
    "                      for w,m in local_models.items()}\n",
    "\n",
    "        trained_models = perform_parallel_training(\n",
    "            datasets, \n",
    "            local_models, \n",
    "            optimizers, \n",
    "            criterions, \n",
    "            model_hyperparams['epochs']\n",
    "        )\n",
    "        \n",
    "        aggregated_params, params, scale_coeffs = calculate_global_params(\n",
    "            global_model, \n",
    "            trained_models, \n",
    "            datasets\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        ============================\n",
    "        * Save states to dictionary\n",
    "\n",
    "        '''\n",
    "        global_model_transfer_out = global_model.get()\n",
    "        global_states.update({rounds : copy.deepcopy(global_model_transfer_out)})\n",
    "        global_model_state_dicts.update({rounds : global_model_transfer_out.state_dict()})\n",
    "            \n",
    "        client_states.update({rounds + 1 : params})\n",
    "\n",
    "        # Update weights with aggregated parameters \n",
    "        global_model_transfer_out.load_state_dict(aggregated_params)\n",
    "#         model = copy.deepcopy(global_model_transfer_out)\n",
    "        client_template = copy.deepcopy(global_model_transfer_out)\n",
    "        global_model = global_model_transfer_out.send(crypto_provider)\n",
    "        \n",
    "        rounds += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "    '''\n",
    "    ============================\n",
    "    * Save final global state\n",
    "\n",
    "    '''\n",
    "    global_model_transfer_out = global_model.get()\n",
    "    global_states.update({rounds : copy.deepcopy(global_model_transfer_out)})\n",
    "    global_model = global_model_transfer_out.send(crypto_provider)\n",
    "    global_model_state_dicts.update({rounds : global_model_transfer_out.state_dict()})\n",
    "    pbar.close()\n",
    "\n",
    "    return global_model, global_states, client_states, scale_coeffs, global_model_state_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Synth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/all_data/data.json') as json_file:\n",
    "\t\tdata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_indices(length, proportions):\n",
    "    train_end = round(length * proportions[0])\n",
    "    val_end = train_end + round(length * proportions[1])\n",
    "    test_end = val_end + round(length * proportions[2])\n",
    "    return train_end, val_end, test_end\n",
    "\n",
    "def prep_synth_data(path, split_proportions):\n",
    "    training_datasets = {}\n",
    "    validation_datasets = {}\n",
    "    testing_datasets = {}\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for client_idx in data:\n",
    "            num_data = len(data[client_idx]['x'])\n",
    "            \n",
    "            train_idx, val_idx, test_idx = get_split_indices(num_data, split_proportions)\n",
    "            \n",
    "            x_and_y = list(zip(data[client_idx]['x'], \n",
    "                               data[client_idx]['y']))\n",
    "            random.shuffle(x_and_y)\n",
    "            x, y = zip(*x_and_y)\n",
    "            training_datasets.update({int(client_idx) : (th.tensor(x[0:train_idx]), \n",
    "                                                         th.tensor(y[0:train_idx]).view(-1, 1))})\n",
    "            validation_datasets.update({int(client_idx) : (th.tensor(x[train_idx:val_idx]), \n",
    "                                                         th.tensor(y[train_idx:val_idx]).view(-1, 1))})\n",
    "            testing_datasets.update({int(client_idx) : (th.tensor(x[val_idx:test_idx]), \n",
    "                                                         th.tensor(y[val_idx:test_idx]).view(-1, 1))})\n",
    "    return training_datasets, validation_datasets, testing_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_testing_datasets(testing_datasets):\n",
    "    output = None\n",
    "    for client in testing_datasets:\n",
    "        if output is None:\n",
    "            output = (testing_datasets[client][0], \n",
    "                      testing_datasets[client][1])\n",
    "        else:\n",
    "            output = (th.cat((output[0], \n",
    "                             testing_datasets[client][0]), \n",
    "                             0), \n",
    "                      th.cat((output[1], \n",
    "                             testing_datasets[client][1]), \n",
    "                             0))\n",
    "    \n",
    "    return (output[0], output[1].view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets, validation_datasets, testing_datasets = prep_synth_data('./data/all_data/data.json', [0.8, 0.1, 0.1])\n",
    "testing_dataset = aggregate_testing_datasets(testing_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FL functions in this notebook convert individual CSVs for each client into a dictionary of tuples, one tuple for each client, where first entry is a torch tensor of data, and second entry is a torch tensor of labels. in set_up_fl_env, indexing data dictionaries is how datasets are disseminated to workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_A(datagen_config,\n",
    "            is_binary,\n",
    "            num_workers, \n",
    "             val_proportion, \n",
    "             test_proportion,\n",
    "             garbage_proportion,\n",
    "             garbage_severity):\n",
    "    \n",
    "    \"\"\" Takes in a synthetic dataset generated via Skl.datasets, \n",
    "        splits it into num_workers even segments, converts them into \n",
    "        torch tensors, formats into a dictionary, for use in FL \n",
    "        training. \n",
    "        \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to split datasets among. \n",
    "        synth_data (tuple(numpy.ndarray, numpy.ndarray)): A synthetic classification dataset - First entry is data, second is labels. \n",
    "        val_proportion (float): Proportion of datapoints to be used for validation. Float between 0 and 1.\n",
    "        test_proportion (float): Proportion of datapoints to be used for testing. Float between 0 and 1. \n",
    "    Returns:\n",
    "        training_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        validation_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        testing_dataset (tuple(torch.Tensor, torch.Tensor)): A tuple containing test data.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(datagen_config)\n",
    "    \n",
    "    synth_data = skld.make_classification(**datagen_config)\n",
    "    \n",
    "    if is_binary:\n",
    "        synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "    \n",
    "    #============\n",
    "    # Number of data points in synthetic data set\n",
    "    synth_data_length = synth_data[0].shape[0]\n",
    "    \n",
    "    #============\n",
    "    # Number of data points per worker\n",
    "    segment_size = math.floor(synth_data_length / num_workers)\n",
    "    \n",
    "    #============\n",
    "    # Initialize dictionaries for holding datasets.\n",
    "    # defaultdict() used to simplify dict modification.\n",
    "    training_datasets = defaultdict()\n",
    "    validation_datasets = defaultdict()\n",
    "    testing_dataset = None\n",
    "    \n",
    "    #============\n",
    "    # Select the workers whose data will be corrupted. \n",
    "    garbage_indices = np.random.choice(num_workers, int(np.floor(num_workers * garbage_proportion)), replace=False)\n",
    "    \n",
    "    for worker_idx in range(num_workers):\n",
    "        \n",
    "        #============\n",
    "        # Number of data points for test, val, train.\n",
    "        test_size = math.floor(test_proportion * segment_size)\n",
    "        val_size = math.floor(val_proportion * segment_size)\n",
    "        train_size = math.floor((1 - val_proportion - test_proportion) * segment_size)\n",
    "        \n",
    "#         print(test_size, val_size, train_size)\n",
    "        \n",
    "        #============\n",
    "        # Start and end index of worker's datasegment out of entire dataset. \n",
    "        start_idx = worker_idx * segment_size\n",
    "        end_idx = (worker_idx + 1) * segment_size\n",
    "        \n",
    "#         print(start_idx, end_idx)\n",
    "        \n",
    "        #============\n",
    "        # Within each segment, the indexes of test, val, train subsegments.\n",
    "        test_idx = start_idx + test_size\n",
    "        val_idx = test_idx + val_size\n",
    "        train_idx = val_idx + train_size\n",
    "        \n",
    "#         print(test_idx, val_idx, train_idx)\n",
    "\n",
    "        #============\n",
    "        # Slice dataset by above segments. Convert to torch tensors. \n",
    "        validation_datasets[worker_idx] = (th.from_numpy(synth_data[0][test_idx:val_idx, :]), \n",
    "                                           th.from_numpy(synth_data[1][test_idx:val_idx]))\n",
    "        \n",
    "        training_datasets[worker_idx] = (th.from_numpy(synth_data[0][val_idx:train_idx, :]), \n",
    "                                         th.from_numpy(synth_data[1][val_idx:train_idx]))\n",
    "        \n",
    "        #============\n",
    "        # Scatter random noise over data matrices of selected workers.\n",
    "        if worker_idx in garbage_indices:\n",
    "            \n",
    "            val_shape = validation_datasets[worker_idx][0].shape\n",
    "            train_shape = training_datasets[worker_idx][0].shape\n",
    "            \n",
    "            val_noise = np.random.rand(val_shape[0], val_shape[1]) * garbage_severity\n",
    "            train_noise = np.random.rand(train_shape[0], train_shape[1]) * garbage_severity\n",
    "            \n",
    "            validation_datasets[worker_idx] = (validation_datasets[worker_idx][0] + th.from_numpy(val_noise), \n",
    "                                               validation_datasets[worker_idx][1])  \n",
    "            training_datasets[worker_idx] = (training_datasets[worker_idx][0] + th.from_numpy(train_noise), \n",
    "                                             training_datasets[worker_idx][1])\n",
    "\n",
    "        \n",
    "        #============\n",
    "        # Testing dataset is in tuple form since it is used by ttp.\n",
    "        # Each segment contributes a portion of the total, hence the\n",
    "        # concatenation. \n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (synth_data[0][start_idx:test_idx, :], \n",
    "                                synth_data[1][start_idx:test_idx])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                synth_data[0][start_idx:test_idx, :]), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                synth_data[1][start_idx:test_idx]), \n",
    "                                               axis = 0))\n",
    "    \n",
    "    #============\n",
    "    # Convert testing dataset to torch tensors. \n",
    "    testing_dataset = (th.from_numpy(testing_dataset[0]), \n",
    "                        th.from_numpy(testing_dataset[1]))\n",
    "        \n",
    "    returned_info_dict = {'garbage_indices': garbage_indices, 'worker_proportions': []}\n",
    "    return training_datasets, validation_datasets, testing_dataset, returned_info_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_B(datagen_config,\n",
    "            is_binary,\n",
    "            num_workers, \n",
    "            val_proportion, \n",
    "            test_proportion,\n",
    "            garbage_proportion,\n",
    "            garbage_severity):\n",
    "    \n",
    "    \"\"\" Takes in a synthetic dataset generated via Skl.datasets, \n",
    "        splits it into num_workers even segments, converts them into \n",
    "        torch tensors, formats into a dictionary, for use in FL \n",
    "        training. \n",
    "        \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to split datasets among. \n",
    "        synth_data (tuple(numpy.ndarray, numpy.ndarray)): A synthetic classification dataset - First entry is data, second is labels. \n",
    "        val_proportion (float): Proportion of datapoints to be used for validation. Float between 0 and 1.\n",
    "        test_proportion (float): Proportion of datapoints to be used for testing. Float between 0 and 1. \n",
    "    Returns:\n",
    "        training_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        validation_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        testing_dataset (tuple(torch.Tensor, torch.Tensor)): A tuple containing test data.\n",
    "    \"\"\"\n",
    "    returned_info_dict = {}\n",
    "    \n",
    "    synth_data = skld.make_classification(**datagen_config)\n",
    "    \n",
    "    if is_binary:\n",
    "        synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "    \n",
    "    worker_proportions = [np.random.random() for i in range(num_workers)]\n",
    "    worker_proportions /= np.sum(worker_proportions)\n",
    "    \n",
    "#     print(worker_proportions)\n",
    "    \n",
    "    #============\n",
    "    # Number of data points in synthetic data set\n",
    "    synth_data_length = synth_data[0].shape[0]\n",
    "    \n",
    "    #============\n",
    "    # Initialize dictionaries for holding datasets.\n",
    "    # defaultdict() used to simplify dict modification.\n",
    "    training_datasets = defaultdict()\n",
    "    validation_datasets = defaultdict()\n",
    "    testing_dataset = None\n",
    "\n",
    "    previous_end_idx = 0\n",
    "    for worker_idx in range(num_workers):\n",
    "        \n",
    "        #============\n",
    "        # Number of data points for test, val, train.\n",
    "        test_size = math.floor((test_proportion * worker_proportions[worker_idx]) * synth_data_length)\n",
    "        val_size = math.floor((val_proportion * worker_proportions[worker_idx]) * synth_data_length)\n",
    "        train_size = math.floor(((1 - val_proportion - test_proportion) * worker_proportions[worker_idx]) * synth_data_length)\n",
    "         \n",
    "#         print(test_size, val_size, train_size)\n",
    "        \n",
    "        #============\n",
    "        # Start and end index of worker's datasegment out of entire dataset. \n",
    "        \n",
    "#         print(worker_proportions[worker_idx] * synth_data_length)\n",
    "        start_idx = previous_end_idx\n",
    "        end_idx = math.floor(previous_end_idx + (worker_proportions[worker_idx] * synth_data_length))\n",
    "        previous_end_idx = end_idx\n",
    "#         print(start_idx, end_idx)\n",
    "        \n",
    "        #============\n",
    "        # Within each segment, the indexes of test, val, train subsegments.\n",
    "        test_idx = start_idx + test_size\n",
    "        val_idx = test_idx + val_size\n",
    "        train_idx = val_idx + train_size\n",
    "        \n",
    "#         print(test_idx, val_idx, train_idx)\n",
    "\n",
    "        #============\n",
    "        # Slice dataset by above segments. Convert to torch tensors. \n",
    "        validation_datasets[worker_idx] = (th.from_numpy(synth_data[0][test_idx:val_idx, :]), \n",
    "                                           th.from_numpy(synth_data[1][test_idx:val_idx]))\n",
    "        \n",
    "        training_datasets[worker_idx] = (th.from_numpy(synth_data[0][val_idx:train_idx, :]), \n",
    "                                         th.from_numpy(synth_data[1][val_idx:train_idx]))\n",
    "        \n",
    "        #============\n",
    "        # Testing dataset is in tuple form since it is used by ttp.\n",
    "        # Each segment contributes a portion of the total, hence the\n",
    "        # concatenation. \n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (synth_data[0][start_idx:test_idx, :], \n",
    "                                synth_data[1][start_idx:test_idx])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                synth_data[0][start_idx:test_idx, :]), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                synth_data[1][start_idx:test_idx]), \n",
    "                                               axis = 0))\n",
    "    \n",
    "    #============\n",
    "    # Convert testing dataset to torch tensors. \n",
    "    testing_dataset = (th.from_numpy(testing_dataset[0]), \n",
    "                        th.from_numpy(testing_dataset[1]))\n",
    "        \n",
    "    returned_info_dict = {'garbage_indices': [], 'worker_proportions': worker_proportions}\n",
    "    return training_datasets, validation_datasets, testing_dataset, returned_info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and Train\n",
    "Set up FL environment, distribute data to participants, perform FL training to produce trained local and global models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounds:   0%|                                                                                   | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model parameters:\n",
      " [<VirtualWorker id:crypto_provider #objects:5>, <VirtualWorker id:crypto_provider #objects:5>, <VirtualWorker id:crypto_provider #objects:5>, <VirtualWorker id:crypto_provider #objects:5>] \n",
      "ID:\n",
      " [63952250482, 1219761808, 72919224206, 97425525780] \n",
      " Cloning effect on global model:\n",
      " [(Wrapper)>[PointerTensor | me:71609300738 -> crypto_provider:63952250482], (Wrapper)>[PointerTensor | me:39724604074 -> crypto_provider:1219761808], (Wrapper)>[PointerTensor | me:50042905899 -> crypto_provider:72919224206], (Wrapper)>[PointerTensor | me:88230722944 -> crypto_provider:97425525780]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounds:   5%|███▊                                                                       | 1/20 [00:18<05:45, 18.18s/it]\u001b[A\n",
      "Rounds:  10%|███████▌                                                                   | 2/20 [00:35<05:20, 17.82s/it]\u001b[A\n",
      "Rounds:  15%|███████████▎                                                               | 3/20 [00:53<05:05, 17.96s/it]\u001b[A\n",
      "Rounds:  20%|███████████████                                                            | 4/20 [01:11<04:47, 17.98s/it]\u001b[A\n",
      "Rounds:  25%|██████████████████▊                                                        | 5/20 [01:29<04:28, 17.88s/it]\u001b[A\n",
      "Rounds:  30%|██████████████████████▌                                                    | 6/20 [01:47<04:11, 17.99s/it]\u001b[A\n",
      "Rounds:  35%|██████████████████████████▎                                                | 7/20 [02:05<03:53, 17.98s/it]\u001b[A\n",
      "Rounds:  40%|██████████████████████████████                                             | 8/20 [02:22<03:34, 17.84s/it]\u001b[A\n",
      "Rounds:  45%|█████████████████████████████████▊                                         | 9/20 [02:40<03:14, 17.67s/it]\u001b[A\n",
      "Rounds:  50%|█████████████████████████████████████                                     | 10/20 [02:58<02:58, 17.84s/it]\u001b[A\n",
      "Rounds:  55%|████████████████████████████████████████▋                                 | 11/20 [03:15<02:38, 17.65s/it]\u001b[A\n",
      "Rounds:  60%|████████████████████████████████████████████▍                             | 12/20 [03:33<02:22, 17.77s/it]\u001b[A\n",
      "Rounds:  65%|████████████████████████████████████████████████                          | 13/20 [03:51<02:04, 17.80s/it]\u001b[A\n",
      "Rounds:  70%|███████████████████████████████████████████████████▊                      | 14/20 [04:08<01:45, 17.63s/it]\u001b[A\n",
      "Rounds:  75%|███████████████████████████████████████████████████████▌                  | 15/20 [04:26<01:28, 17.69s/it]\u001b[A\n",
      "Rounds:  80%|███████████████████████████████████████████████████████████▏              | 16/20 [04:44<01:11, 17.76s/it]\u001b[A\n",
      "Rounds:  85%|██████████████████████████████████████████████████████████████▉           | 17/20 [05:01<00:52, 17.58s/it]\u001b[A\n",
      "Rounds:  90%|██████████████████████████████████████████████████████████████████▌       | 18/20 [05:18<00:34, 17.49s/it]\u001b[A\n",
      "Rounds:  95%|██████████████████████████████████████████████████████████████████████▎   | 19/20 [05:37<00:17, 17.72s/it]\u001b[A\n",
      "Rounds: 100%|██████████████████████████████████████████████████████████████████████████| 20/20 [05:54<00:00, 17.73s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# ============\n",
    "# Hyperparams for FL model. \n",
    "\n",
    "#============\n",
    "# Binary Class hyperparams \n",
    "binary_model_hyperparams = {\n",
    "    \"batch_size\": 45,\n",
    "    \"lr\": 0.1,\n",
    "    \"decay\": 0.01,\n",
    "    \"rounds\":20,\n",
    "    \"epochs\": 20,\n",
    "    \"criterion\": nn.BCELoss,\n",
    "    \"is_condensed\": True\n",
    "}\n",
    "\n",
    "#============\n",
    "# Multi Class hyperparams \n",
    "multiclass_model_hyperparams = {\n",
    "    \"batch_size\": 45,\n",
    "    \"lr\": 0.01,\n",
    "    \"decay\": 0.1,\n",
    "    \"rounds\":1,\n",
    "    \"epochs\": 1,\n",
    "    \"criterion\": nn.CrossEntropyLoss,\n",
    "    \"is_condensed\": False\n",
    "}\n",
    "\n",
    "binary_model_structure =  {\n",
    "    '0':{'layer_size_mapping': {\"in_features\": 5,\n",
    "                                \"out_features\": 64},\n",
    "        \"layer_type\": 'linear',\n",
    "        \"activation\": 'sigmoid'}, \n",
    "    '1':{'layer_size_mapping': {\"in_features\": 64,\n",
    "                                \"out_features\": 1},\n",
    "        \"layer_type\": 'linear',\n",
    "        \"activation\": 'sigmoid'},   \n",
    "}\n",
    "\n",
    "# multiclass_model_structure =  {\n",
    "#     '0':{\"in_features\": 20,\n",
    "#         \"out_features\": 200,\n",
    "#         \"layer_type\": 'linear',\n",
    "#         \"activation\": 'sigmoid'}, \n",
    "#     '1':{\"in_features\": 200,\n",
    "#         \"out_features\": 4,\n",
    "#         \"layer_type\": 'linear',\n",
    "#         \"activation\": 'nil'},   \n",
    "# }\n",
    "\n",
    "# {'in_channels': 10, \n",
    "#      'out_channels': 10, \n",
    "#      'kernel_size': 10,\n",
    "# }\n",
    "\n",
    "model_hyperparams = binary_model_hyperparams\n",
    "\n",
    "#============\n",
    "# Set up Federated Learning environment.\n",
    "# Produce points to the datasets stored on workers \n",
    "# and ttp. Also produce pointers to workers and ttp.\n",
    "\n",
    "(training_pointers, \n",
    " validation_pointers, \n",
    " testing_pointer, \n",
    " workers, \n",
    " crypto_provider) = setup_FL_env(\n",
    "    training_datasets,\n",
    "    validation_datasets, \n",
    "    testing_dataset\n",
    ")\n",
    "\n",
    "#=============\n",
    "# Produce individual trainlaoders for each client\n",
    "# which make use of the full data available to them. \n",
    "trainloaders = {}\n",
    "for worker_id in list(training_pointers.keys()):\n",
    "    train = {worker_id:training_pointers[worker_id]}\n",
    "    val = {worker_id:validation_pointers[worker_id]}\n",
    "    #============\n",
    "    # Convert training datasets into syft dataloaders. \n",
    "    train_loader, validation_loader, test_loader = convert_to_FL_batches(\n",
    "        binary_model_hyperparams,\n",
    "        train, \n",
    "        val, \n",
    "        testing_pointer\n",
    "    )\n",
    "    trainloaders.update({worker_id : train_loader})\n",
    "    \n",
    "# for key in trainloaders: \n",
    "#     print(key)\n",
    "#     for batch_idx, batch in enumerate(trainloaders[key]):\n",
    "#         print(batch[1].shape)\n",
    "        \n",
    "# #============\n",
    "# # Commence FL training and return trained global model, as well as \n",
    "# global and local model states over time. \n",
    "\n",
    "trained_model, global_states, client_states, scale_coeffs, global_model_state_dicts = perform_FL_training(\n",
    "    model_hyperparams,\n",
    "    binary_model_structure,\n",
    "    trainloaders,\n",
    "    workers,\n",
    "    crypto_provider\n",
    ")\n",
    "\n",
    "# # trainloaders\n",
    "# print(scale_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation function for CC\n",
    "This testing function is temporary;\n",
    "        It is used in contribution calculations with the assumption \n",
    "        that the TTP has unfettered access to a testing dataset. This\n",
    "        may not be in the case in non-simulation environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Libs\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import sklearn.datasets as skld\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Contribution_Calculation:\n",
    "    def __init__(self, global_states, model_hyperparams, client_state_dict, testing_dataset, scale_coeffs):\n",
    "        self.global_states = global_states\n",
    "        self.model_hyperparams = model_hyperparams\n",
    "        self.client_state_dict = client_state_dict\n",
    "        self.testing_dataset = testing_dataset\n",
    "        self.scale_coeffs = scale_coeffs\n",
    "\n",
    "    def perform_FL_testing(self, model):\n",
    "        \"\"\" Obtains predictions given a validation/test dataset upon\n",
    "            a given PyTorch model.\n",
    "\n",
    "        Args:\n",
    "            model   (nn.Module): A PyTorch model\n",
    "        Returns:\n",
    "            accuracy score (float)\n",
    "            roc_auc score  (float)\n",
    "        \"\"\"\n",
    "        X_test = self.testing_dataset[0]\n",
    "        y_test = self.testing_dataset[1]\n",
    "        model.eval()\n",
    "        with th.no_grad():\n",
    "            predicted_labels = model(X_test.float())\n",
    "            if self.model_hyperparams['is_condensed']:\n",
    "                accuracy = accuracy_score(y_test.numpy(), predicted_labels.round().numpy())\n",
    "                roc = roc_auc_score(y_test.numpy(), predicted_labels.numpy())\n",
    "            else:\n",
    "                accuracy = accuracy_score(y_test.numpy(), np.array([np.argmax(i) for i in predicted_labels.numpy()]))\n",
    "                roc = roc_auc_score(y_test.numpy(), np.array([softmax(i) for i in predicted_labels.numpy()]), multi_class='ovr')\n",
    "        return accuracy, roc\n",
    "\n",
    "    def prep_client_state_dict(self):\n",
    "        \"\"\" Takes in the client states dictionary returned by training function.\n",
    "            Returns dictionary initialized with empty dictionaries for each client\n",
    "            at each timestep.\n",
    "\n",
    "        Args:\n",
    "            client_states (dict): Client states at each communication round.\n",
    "        Returns:\n",
    "            client_dict (dict): Dictionary initialized to be filled with actual model state\n",
    "                                tensors.\n",
    "        \"\"\"\n",
    "        client_dict = {}\n",
    "        for timestep in self.client_states.keys():\n",
    "            client_dict.update({timestep : {}})\n",
    "            for param_type in self.client_states[timestep].keys():\n",
    "                for client_idx in range(len(self.client_states[timestep][param_type])):\n",
    "                    client_dict[timestep].update({client_idx : {}})\n",
    "                break\n",
    "        self.client_state_dict = client_dict\n",
    "\n",
    "    def fill_client_state_dict(self):\n",
    "        \"\"\" Populates prepared client state dictionary with model states\n",
    "            for each client at each timestep.\n",
    "\n",
    "        Args:\n",
    "            client_states (dict): Client states at each communication round.\n",
    "        Returns:\n",
    "            client_state_dict (dict): Complete state dictionary\n",
    "        \"\"\"\n",
    "        self.prep_client_state_dict(self.client_states)\n",
    "        for timestep in self.client_states.keys():\n",
    "            for param_type in self.client_states[timestep].keys():\n",
    "                for client_idx in range(len(self.client_states[timestep][param_type])):\n",
    "                    self.client_dict[timestep][client_idx].update({param_type : self.client_states[timestep][param_type][client_idx]})\n",
    "\n",
    "    def index_scale_coeffs_by_integer(self):\n",
    "        \"\"\" scale_coeffs are indexed by FL worker id. Since CC functions\n",
    "            below rely on integer indexing of clients, need to reindex\n",
    "            keys while keeping values constant.\n",
    "\n",
    "        Args:\n",
    "            scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                        on number of datapoints, indexed by\n",
    "                                        FL worker ID.\n",
    "        Returns:\n",
    "            scale_dictionary (dict): Identical to scale_coeffs, except indexed\n",
    "                                     by integers 0, 1, 2, ... etc.\n",
    "        \"\"\"\n",
    "        scale_dictionary = {}\n",
    "        for i, key in enumerate(self.scale_coeffs.keys()):\n",
    "            scale_dictionary.update({i : self.scale_coeffs[key]})\n",
    "        return scale_dictionary\n",
    "\n",
    "    def produce_excluded_scaling_factor(self, idx):\n",
    "        \"\"\" In aggregation of client subsets, scale_coeffs need to be recalculated.\n",
    "            For example. Three clients. Each contributes 0.33 based on their dataset\n",
    "            size. I exclude one. When I update global model, I should weight the\n",
    "            remaining updates by 0.5 instead of 0.33. This function produces a scale\n",
    "            factor which will divide the coefficients prior to aggregation.\n",
    "\n",
    "        Args:\n",
    "            indexed_scale_coeffs (OrderedDict): Output of above function.\n",
    "        Returns:\n",
    "            coeffs_scale_factor (float): The sum of scale_coeffs for non-excluded clients.\n",
    "        \"\"\"\n",
    "        coeffs_scale_factor = 0\n",
    "        for i in indexed_scale_coeffs:\n",
    "            if i != idx:\n",
    "                coeffs_scale_factor += indexed_scale_coeffs[i]\n",
    "        return coeffs_scale_factor\n",
    "\n",
    "    def prep_dl_dw(self):\n",
    "        dl_dw = {}\n",
    "        for layer_name in self.client_state_dict[1][0].keys():\n",
    "            dl_dw.update({layer_name : th.zeros(self.client_state_dict[1][0][layer_name].shape, dtype=th.float64)})\n",
    "        return dl_dw\n",
    "\n",
    "    def calculate_dl_dw(self, global_state, rnd, idx):\n",
    "        dl_dw = self.prep_dl_dw()\n",
    "        for layer_name in global_state.state_dict().keys():\n",
    "            dl_dw[layer_name] = self.client_state_dict[rnd][idx][layer_name] - global_state.state_dict()[layer_name]\n",
    "        return dl_dw\n",
    "\n",
    "    def calculate_GRV(self, final_state, current_state):\n",
    "        diff = final_state\n",
    "        for layer_name in final_state.state_dict().keys():\n",
    "            diff.state_dict()[layer_name] -= current_state.state_dict()[layer_name]\n",
    "        return diff\n",
    "\n",
    "    def calculate_alignment(self, dl_dw, GRV):\n",
    "        alignment = 0\n",
    "        num_layers = 0\n",
    "        for layer_name in dl_dw:\n",
    "            alignment += th.dot(th.flatten(GRV.state_dict()[layer_name]), th.flatten(dl_dw[layer_name]))\n",
    "            num_layers += 1\n",
    "        return alignment/num_layers\n",
    "\n",
    "    def singular(self,\n",
    "                 reference_eval,\n",
    "                 rnd,\n",
    "                 client_idx):\n",
    "        \"\"\"\n",
    "        This function evaluates the client's contribution to performance metrics\n",
    "        in isolation from those of other clients.\n",
    "\n",
    "        Args:\n",
    "            global_states (dict): Dictionary of global model states.\n",
    "            client_state_dict (dict): Dictionary of client model states at each timestep.\n",
    "            testing_dataset (th.tensor): The testing dataset\n",
    "            reference_eval (float): The accuracy and ROC-AUC score of the baseline model\n",
    "                                    at timestep rnd.\n",
    "            rnd (int): The current round\n",
    "            client_idx (int): The index of the client being evaluated\n",
    "            scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                        on number of datapoints, indexed by\n",
    "                                        FL worker ID.\n",
    "        Returns:\n",
    "            Difference between performance of client model and global model.\n",
    "        \"\"\"\n",
    "        single_client_model = copy.deepcopy(self.global_states[rnd - 1])\n",
    "        single_client_model.load_state_dict(self.client_state_dict[rnd][client_idx])\n",
    "\n",
    "        client_eval = self.perform_FL_testing(single_client_model)\n",
    "        print('---------')\n",
    "        print(f\"Improvement of {client_idx}'s update in isolation when applied to global model at round {rnd - 1}\")\n",
    "        print((client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1]))\n",
    "        return (client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1])\n",
    "\n",
    "    def aggregate(self,\n",
    "                  reference_eval,\n",
    "                  rnd,\n",
    "                  client_idx,\n",
    "                  client_subset_size = 1.0):\n",
    "        \"\"\"\n",
    "        This function evaluates the client's contribution to performance metrics\n",
    "        in isolation from those of other clients.\n",
    "\n",
    "        Args:\n",
    "            global_states (dict): Dictionary of global model states.\n",
    "            client_state_dict (dict): Dictionary of client model states at each timestep.\n",
    "            testing_dataset (th.tensor): The testing dataset\n",
    "            reference_eval (float): The accuracy and ROC-AUC score of the baseline model\n",
    "                                    at timestep rnd.\n",
    "            rnd (int): The current round\n",
    "            client_idx (int): The index of the client being evaluated\n",
    "            scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                        on number of datapoints, indexed by\n",
    "                                        FL worker ID.\n",
    "        Returns:\n",
    "            Difference between performance of client model and global model.\n",
    "        \"\"\"\n",
    "        aggregate_exclusion_model = copy.deepcopy(self.global_states[rnd - 1])\n",
    "\n",
    "        dl_dw = self.prep_dl_dw()\n",
    "\n",
    "        indexed_scale_coeffs = self.index_scale_coeffs_by_integer()\n",
    "        scaling_soeff_factor = self.produce_excluded_scaling_factor(client_idx)\n",
    "\n",
    "        num_clients = len(self.client_state_dict[1].keys())\n",
    "        client_indices = np.random.choice(num_clients, int(np.floor(num_clients * client_subset_size)), replace=False)\n",
    "\n",
    "        for client in client_indices:\n",
    "            if client != client_idx:\n",
    "                for layer_name in self.client_state_dict[rnd][client]:\n",
    "                    dl_dw[layer_name] += (indexed_scale_coeffs[client]/scaling_soeff_factor) * self.client_state_dict[rnd][client][layer_name]\n",
    "\n",
    "        aggregate_states = aggregate_exclusion_model.state_dict()\n",
    "        for param_type in dl_dw:\n",
    "            aggregate_states[param_type] += dl_dw[param_type]\n",
    "        aggregate_exclusion_model.load_state_dict(aggregate_states)\n",
    "\n",
    "        client_eval = self.perform_FL_testing(aggregate_exclusion_model)\n",
    "        \n",
    "        print('---------')\n",
    "        print(f\"Difference in performance of global model at round {rnd - 1} when client {client_idx} is excluded.\")\n",
    "        print((client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1]))\n",
    "\n",
    "        return (client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1])\n",
    "\n",
    "    def contribution_calculation(self, del_method):\n",
    "        \"\"\"\n",
    "        This function calculates the contribution of each client to model\n",
    "        training using the deletion and alignment methods.\n",
    "\n",
    "        Args:\n",
    "            model_hyperparams (dict): Hyperparams used in FL model training.\n",
    "            global_states (dict): Dictionary of global model states.\n",
    "            client_state_dict (dict): Dictionary of client model states at each timestep.\n",
    "            testing_dataset (th.tensor): The testing dataset.\n",
    "            del_method (string): Choice of singular or aggregate deletion method.\n",
    "            scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                        on number of datapoints, indexed by\n",
    "                                        FL worker ID.\n",
    "        Returns:\n",
    "            client_alignment_matrix (np.array): Client alignments at each timestep.\n",
    "            client_deletion_matrix (np.array): Differences in client performance at each timestep.\n",
    "        \"\"\"\n",
    "        total_num_rounds = self.model_hyperparams['rounds']\n",
    "\n",
    "        final_global_state = selff.global_states[total_num_rounds]\n",
    "        client_alignment_matrix = np.zeros([len(self.client_state_dict[1].keys()), total_num_rounds], dtype=np.float64)\n",
    "        client_deletion_matrix = np.zeros([len(self.client_state_dict[1].keys()), total_num_rounds], dtype=np.float64)\n",
    "\n",
    "        del_dict = {'Singular' : self.singular,\n",
    "                   'Aggregate' : self.aggregate}\n",
    "\n",
    "        for rnd in range(1, total_num_rounds + 1):\n",
    "            current_global_state = self.global_states[rnd - 1]\n",
    "\n",
    "            GRV = self.calculate_GRV(final_global_state, current_global_state)\n",
    "\n",
    "            reference_eval = self.perform_FL_testing(current_global_state)\n",
    "\n",
    "            print('----------')\n",
    "            print(f'Performance of global model at timestep {rnd-1}')\n",
    "            print(reference_eval)\n",
    "\n",
    "            for client_idx in self.client_state_dict[rnd].keys():\n",
    "\n",
    "    #             ============\n",
    "    #             Calculate alignment of dl/dw with GRV\n",
    "    #             print(client_state_dict[rnd][client_idx])\n",
    "                dl_dw = self.calculate_dl_dw(current_global_state, rnd, client_idx)\n",
    "                alignment = self.calculate_alignment(dl_dw, GRV)\n",
    "                print('---------')\n",
    "                print(f'Alignment of client {client_idx} with GRV at round {rnd}')\n",
    "                print(alignment.numpy())\n",
    "                client_alignment_matrix[client_idx][rnd - 1] = alignment.numpy()\n",
    "\n",
    "                #============\n",
    "                # Calculate change in model performance metrics when client contribution is\n",
    "                # selectively deleted.\n",
    "                client_eval = del_dict[del_method](global_states,\n",
    "                                                   client_state_dict,\n",
    "                                                   testing_dataset,\n",
    "                                                   reference_eval,\n",
    "                                                   rnd,\n",
    "                                                   client_idx,\n",
    "                                                   scale_coeffs,\n",
    "                                                   model_hyperparams)\n",
    "                client_deletion_matrix[client_idx][rnd - 1] = (client_eval[0] + client_eval[1])\n",
    "\n",
    "        print('===============')\n",
    "        print('Client alignment matrix')\n",
    "        print(client_alignment_matrix)\n",
    "        print('===============')\n",
    "        print('Client deletion matrix')\n",
    "        print(client_deletion_matrix)\n",
    "\n",
    "        return client_alignment_matrix, client_deletion_matrix\n",
    "\n",
    "    def normalize_contribution_matrix(mat):\n",
    "        return (mat - np.mean(mat)) / np.std(mat)\n",
    "\n",
    "    def aggregate_contribution_matrices(arguments, align_mat, del_mat):\n",
    "    #     align_mat = normalize_contribution_matrix(align_mat)\n",
    "    #     del_mat = normalize_contribution_matrix(del_mat)\n",
    "\n",
    "        contributions = defaultdict()\n",
    "\n",
    "        for i in range(align_mat.shape[0]):\n",
    "            contributions[i] = np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds']\n",
    "    #         print(np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds'])\n",
    "            print(np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds'])\n",
    "        return contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Contribution_Calculation(global_states, model_hyperparams, client_states, testing_dataset, scale_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Contribution_Calculation' object has no attribute 'client_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-da7cc4eea2ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_client_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-a2e3bd085e8c>\u001b[0m in \u001b[0;36mfill_client_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mclient_state_dict\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mComplete\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \"\"\"\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_client_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mparam_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Contribution_Calculation' object has no attribute 'client_states'"
     ]
    }
   ],
   "source": [
    "cc.fill_client_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_alignment_matrix, client_deletion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def perform_FL_testing(dataset, model, model_hyperparams): \n",
    "    \"\"\" Obtains predictions given a validation/test dataset upon \n",
    "        a given PyTorch model. \n",
    "        \n",
    "    Args:\n",
    "        model   (nn.Module): A PyTorch model\n",
    "    Returns:\n",
    "        accuracy score (float)\n",
    "        roc_auc score  (float)\n",
    "    \"\"\"\n",
    "    X_test = dataset[0]\n",
    "    y_test = dataset[1]\n",
    "#     print(X_test)\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        predicted_labels = model(X_test.float())\n",
    "#         print(predicted_labels.shape)\n",
    "#         print(y_test.shape)\n",
    "        if model_hyperparams['is_condensed']:\n",
    "            accuracy = accuracy_score(y_test.numpy(), predicted_labels.round().numpy())\n",
    "            roc = roc_auc_score(y_test.numpy(), predicted_labels.numpy())\n",
    "        else:\n",
    "            accuracy = accuracy_score(y_test.numpy(), np.array([np.argmax(i) for i in predicted_labels.numpy()]))\n",
    "            roc = roc_auc_score(y_test.numpy(), np.array([softmax(i) for i in predicted_labels.numpy()]), multi_class='ovr')\n",
    "    return accuracy, roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC Prep Functions\n",
    "\n",
    "Need to reorganize client states to index first by communication round, then by client, then by param type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_client_state_dict(client_states):\n",
    "    \"\"\" Takes in the client states dictionary returned by training function.\n",
    "        Returns dictionary initialized with empty dictionaries for each client\n",
    "        at each timestep. \n",
    "        \n",
    "    Args:\n",
    "        client_states (dict): Client states at each communication round. \n",
    "    Returns:\n",
    "        client_dict (dict): Dictionary initialized to be filled with actual model state\n",
    "                            tensors. \n",
    "    \"\"\"\n",
    "    client_dict = {}\n",
    "    for timestep in client_states.keys():\n",
    "        client_dict.update({timestep : {}})\n",
    "        for param_type in client_states[timestep].keys():\n",
    "            for client_idx in range(len(client_states[timestep][param_type])):\n",
    "                client_dict[timestep].update({client_idx : {}})\n",
    "            break\n",
    "    return client_dict\n",
    "\n",
    "def fill_client_state_dict(client_states):\n",
    "    \"\"\" Populates prepared client state dictionary with model states\n",
    "        for each client at each timestep. \n",
    "        \n",
    "    Args:\n",
    "        client_states (dict): Client states at each communication round. \n",
    "    Returns:\n",
    "        client_state_dict (dict): Complete state dictionary\n",
    "    \"\"\"\n",
    "    client_state_dict = prep_client_state_dict(client_states)\n",
    "    for timestep in client_states.keys():\n",
    "        for param_type in client_states[timestep].keys():\n",
    "            for client_idx in range(len(client_states[timestep][param_type])):\n",
    "                client_state_dict[timestep][client_idx].update({param_type : client_states[timestep][param_type][client_idx]})\n",
    "    return client_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_scale_coeffs_by_integer(scale_coeffs):\n",
    "    \"\"\" scale_coeffs are indexed by FL worker id. Since CC functions\n",
    "        below rely on integer indexing of clients, need to reindex\n",
    "        keys while keeping values constant.\n",
    "        \n",
    "    Args:\n",
    "        scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                    on number of datapoints, indexed by\n",
    "                                    FL worker ID. \n",
    "    Returns:\n",
    "        scale_dictionary (dict): Identical to scale_coeffs, except indexed\n",
    "                                 by integers 0, 1, 2, ... etc.\n",
    "    \"\"\"\n",
    "    scale_dictionary = {}\n",
    "    for i, key in enumerate(scale_coeffs.keys()):\n",
    "        scale_dictionary.update({i : scale_coeffs[key]})\n",
    "    return scale_dictionary\n",
    "\n",
    "def produce_excluded_scaling_factor(indexed_scale_coeffs, idx):\n",
    "    \"\"\" In aggregation of client subsets, scale_coeffs need to be recalculated.\n",
    "        For example. Three clients. Each contributes 0.33 based on their dataset \n",
    "        size. I exclude one. When I update global model, I should weight the\n",
    "        remaining updates by 0.5 instead of 0.33. This function produces a scale\n",
    "        factor which will divide the coefficients prior to aggregation. \n",
    "        \n",
    "    Args:\n",
    "        indexed_scale_coeffs (OrderedDict): Output of above function.\n",
    "    Returns:\n",
    "        coeffs_scale_factor (float): The sum of scale_coeffs for non-excluded clients.\n",
    "    \"\"\"\n",
    "    coeffs_scale_factor = 0\n",
    "    for i in indexed_scale_coeffs:\n",
    "        if i != idx:\n",
    "            coeffs_scale_factor += indexed_scale_coeffs[i]\n",
    "    return coeffs_scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dl_dw(client_state_dict):\n",
    "    dl_dw = {}\n",
    "    for layer_name in client_state_dict[1][0].keys():\n",
    "        dl_dw.update({layer_name : th.zeros(client_state_dict[1][0][layer_name].shape, dtype=th.float64)})\n",
    "    return dl_dw\n",
    "\n",
    "def calculate_dl_dw(client_state_dict, global_state, rnd, idx):\n",
    "    dl_dw = prep_dl_dw(client_state_dict)\n",
    "    for layer_name in global_state.state_dict().keys():\n",
    "        dl_dw[layer_name] = client_state_dict[rnd][idx][layer_name] - global_state.state_dict()[layer_name]\n",
    "    return dl_dw\n",
    "\n",
    "def calculate_GRV(final_state, current_state):\n",
    "    diff = final_state\n",
    "    for layer_name in final_state.state_dict().keys():\n",
    "        diff.state_dict()[layer_name] -= current_state.state_dict()[layer_name]\n",
    "    return diff\n",
    "\n",
    "def calculate_alignment(dl_dw, GRV):\n",
    "    alignment = 0\n",
    "    num_layers = 0\n",
    "    for layer_name in dl_dw:\n",
    "        alignment += th.dot(th.flatten(GRV.state_dict()[layer_name]), th.flatten(dl_dw[layer_name]))\n",
    "        num_layers += 1\n",
    "    return alignment/num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular(global_states, \n",
    "             client_state_dict, \n",
    "             testing_dataset, \n",
    "             reference_eval, \n",
    "             rnd, \n",
    "             client_idx, \n",
    "             scale_coeffs,\n",
    "             model_hyperparams,\n",
    "             client_subset_size = 1.0):\n",
    "    \"\"\" \n",
    "    This function evaluates the client's contribution to performance metrics\n",
    "    in isolation from those of other clients. \n",
    "        \n",
    "    Args:\n",
    "        global_states (dict): Dictionary of global model states. \n",
    "        client_state_dict (dict): Dictionary of client model states at each timestep. \n",
    "        testing_dataset (th.tensor): The testing dataset\n",
    "        reference_eval (float): The accuracy and ROC-AUC score of the baseline model\n",
    "                                at timestep rnd. \n",
    "        rnd (int): The current round\n",
    "        client_idx (int): The index of the client being evaluated\n",
    "        scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                    on number of datapoints, indexed by\n",
    "                                    FL worker ID. \n",
    "    Returns:\n",
    "        Difference between performance of client model and global model. \n",
    "    \"\"\"\n",
    "    single_client_model = copy.deepcopy(global_states[rnd - 1])\n",
    "    single_client_model.load_state_dict(client_state_dict[rnd][client_idx])\n",
    "    \n",
    "    client_eval = perform_FL_testing(testing_dataset, \n",
    "                                     single_client_model, \n",
    "                                     model_hyperparams)\n",
    "    print('---------')\n",
    "    print(f\"Improvement of {client_idx}'s update in isolation when applied to global model at round {rnd - 1}\")\n",
    "    print((client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1]))\n",
    "    return (client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1])\n",
    "\n",
    "def aggregate(global_states, \n",
    "              client_state_dict, \n",
    "              testing_dataset, \n",
    "              reference_eval, \n",
    "              rnd,\n",
    "              client_idx, \n",
    "              scale_coeffs, \n",
    "              model_hyperparams,\n",
    "              client_subset_size = 1.0):\n",
    "    \"\"\" \n",
    "    This function evaluates the client's contribution to performance metrics\n",
    "    in isolation from those of other clients. \n",
    "        \n",
    "    Args:\n",
    "        global_states (dict): Dictionary of global model states. \n",
    "        client_state_dict (dict): Dictionary of client model states at each timestep. \n",
    "        testing_dataset (th.tensor): The testing dataset\n",
    "        reference_eval (float): The accuracy and ROC-AUC score of the baseline model\n",
    "                                at timestep rnd. \n",
    "        rnd (int): The current round\n",
    "        client_idx (int): The index of the client being evaluated\n",
    "        scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                    on number of datapoints, indexed by\n",
    "                                    FL worker ID. \n",
    "    Returns:\n",
    "        Difference between performance of client model and global model. \n",
    "    \"\"\"\n",
    "    aggregate_exclusion_model = copy.deepcopy(global_states[rnd - 1])\n",
    "    \n",
    "    dl_dw = prep_dl_dw(client_state_dict)\n",
    "\n",
    "    indexed_scale_coeffs = index_scale_coeffs_by_integer(scale_coeffs)\n",
    "    scaling_soeff_factor = produce_excluded_scaling_factor(indexed_scale_coeffs, client_idx)\n",
    "    \n",
    "    num_clients = len(client_state_dict[1].keys())\n",
    "    client_indices = np.random.choice(num_clients, int(np.floor(num_clients * client_subset_size)), replace=False)\n",
    "    \n",
    "    for client in client_indices:\n",
    "        \n",
    "        if client != client_idx:\n",
    "            for layer_name in client_state_dict[rnd][client]:\n",
    "                dl_dw[layer_name] += (indexed_scale_coeffs[client]/scaling_soeff_factor) * client_state_dict[rnd][client][layer_name]\n",
    "                \n",
    "    aggregate_states = aggregate_exclusion_model.state_dict()\n",
    "    for param_type in dl_dw:\n",
    "        aggregate_states[param_type] += dl_dw[param_type]\n",
    "    aggregate_exclusion_model.load_state_dict(aggregate_states)\n",
    "    \n",
    "    client_eval = perform_FL_testing(testing_dataset, \n",
    "                                     aggregate_exclusion_model, \n",
    "                                     model_hyperparams)\n",
    "    print('---------')\n",
    "    print(f\"Difference in performance of global model at round {rnd - 1} when client {client_idx} is excluded.\")\n",
    "    print((client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1]))\n",
    "    \n",
    "    return (client_eval[0] - reference_eval[0], client_eval[1] - reference_eval[1])\n",
    "    \n",
    "def contribution_calculation(model_hyperparams, global_states, client_state_dict, testing_dataset, del_method, scale_coeffs):\n",
    "    \"\"\" \n",
    "    This function calculates the contribution of each client to model \n",
    "    training using the deletion and alignment methods. \n",
    "        \n",
    "    Args:\n",
    "        model_hyperparams (dict): Hyperparams used in FL model training. \n",
    "        global_states (dict): Dictionary of global model states. \n",
    "        client_state_dict (dict): Dictionary of client model states at each timestep. \n",
    "        testing_dataset (th.tensor): The testing dataset.\n",
    "        del_method (string): Choice of singular or aggregate deletion method. \n",
    "        scale_coeffs (OrderedDict): Scaling factors for each client based\n",
    "                                    on number of datapoints, indexed by\n",
    "                                    FL worker ID. \n",
    "    Returns:\n",
    "        client_alignment_matrix (np.array): Client alignments at each timestep. \n",
    "        client_deletion_matrix (np.array): Differences in client performance at each timestep. \n",
    "    \"\"\"\n",
    "    total_num_rounds = model_hyperparams['rounds']\n",
    "    \n",
    "    final_global_state = global_states[total_num_rounds]\n",
    "    client_alignment_matrix = np.zeros([len(client_state_dict[1].keys()), total_num_rounds], dtype=np.float64)\n",
    "    client_deletion_matrix = np.zeros([len(client_state_dict[1].keys()), total_num_rounds], dtype=np.float64)\n",
    "    \n",
    "    del_dict = {'Singular' : singular,\n",
    "               'Aggregate' : aggregate}\n",
    "    \n",
    "    for rnd in range(1, total_num_rounds + 1):\n",
    "        current_global_state = global_states[rnd - 1]\n",
    "        \n",
    "        GRV = calculate_GRV(final_global_state, current_global_state)\n",
    "        \n",
    "        reference_eval = perform_FL_testing(testing_dataset, current_global_state, model_hyperparams)\n",
    "        \n",
    "        print('----------')\n",
    "        print(f'Performance of global model at timestep {rnd-1}')\n",
    "        print(reference_eval)\n",
    "        \n",
    "        for client_idx in client_state_dict[rnd].keys():\n",
    "            \n",
    "#             ============\n",
    "#             Calculate alignment of dl/dw with GRV\n",
    "#             print(client_state_dict[rnd][client_idx])\n",
    "            dl_dw = calculate_dl_dw(client_state_dict, current_global_state, rnd, client_idx)\n",
    "            alignment = calculate_alignment(dl_dw, GRV)\n",
    "            print('---------')\n",
    "            print(f'Alignment of client {client_idx} with GRV at round {rnd}')\n",
    "            print(alignment.numpy())\n",
    "            client_alignment_matrix[client_idx][rnd - 1] = alignment.numpy()\n",
    "            \n",
    "            #============\n",
    "            # Calculate change in model performance metrics when client contribution is \n",
    "            # selectively deleted.\n",
    "            client_eval = del_dict[del_method](global_states, \n",
    "                                               client_state_dict, \n",
    "                                               testing_dataset, \n",
    "                                               reference_eval, \n",
    "                                               rnd, \n",
    "                                               client_idx,\n",
    "                                               scale_coeffs, \n",
    "                                               model_hyperparams)\n",
    "            client_deletion_matrix[client_idx][rnd - 1] = (client_eval[0] + client_eval[1])\n",
    "\n",
    "    print('===============')\n",
    "    print('Client alignment matrix')\n",
    "    print(client_alignment_matrix)\n",
    "    print('===============')\n",
    "    print('Client deletion matrix')\n",
    "    print(client_deletion_matrix)\n",
    "    \n",
    "    return client_alignment_matrix, client_deletion_matrix\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([318, 5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([478, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_model_hyperparams['is_condensed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_state_dict = fill_client_state_dict(client_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Performance of global model at timestep 0\n",
      "(0.63, 0.019305019305019298)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 1\n",
      "3.81867\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 0\n",
      "(0.16000000000000003, 0.9776919776919777)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 1\n",
      "4.3231726\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 0\n",
      "(0.31999999999999995, 0.9772629772629773)\n",
      "----------\n",
      "Performance of global model at timestep 1\n",
      "(0.97, 0.996996996996997)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 2\n",
      "5.0260267\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 1\n",
      "(-0.15000000000000002, 0.00042900042900040347)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 2\n",
      "5.2369576\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 1\n",
      "(-0.020000000000000018, 0.00042900042900040347)\n",
      "----------\n",
      "Performance of global model at timestep 2\n",
      "(0.96, 0.9974259974259974)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 3\n",
      "7.656387\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 2\n",
      "(-0.17999999999999994, 1.1102230246251565e-16)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 3\n",
      "7.7590895\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 2\n",
      "(-0.010000000000000009, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 3\n",
      "(0.96, 0.9974259974259974)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 4\n",
      "10.08058\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 3\n",
      "(-0.16999999999999993, 1.1102230246251565e-16)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 4\n",
      "10.1025915\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 3\n",
      "(-0.010000000000000009, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 4\n",
      "(0.97, 0.9974259974259975)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 5\n",
      "12.327118\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 4\n",
      "(-0.12, 0.0)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 5\n",
      "12.236008\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 4\n",
      "(-0.020000000000000018, -1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 5\n",
      "(0.96, 0.9974259974259975)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 6\n",
      "14.406538\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 5\n",
      "(-0.13, 0.0)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 6\n",
      "14.196181\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 5\n",
      "(0.010000000000000009, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 6\n",
      "(0.96, 0.9974259974259974)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 7\n",
      "16.348015\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 6\n",
      "(-0.010000000000000009, 0.00042900042900040347)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 7\n",
      "16.098965\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 6\n",
      "(0.010000000000000009, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 7\n",
      "(0.97, 0.9974259974259975)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 8\n",
      "18.471054\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 7\n",
      "(-0.14, 0.0)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 8\n",
      "18.092543\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 7\n",
      "(-0.010000000000000009, -1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 8\n",
      "(0.96, 0.9974259974259975)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 9\n",
      "20.598633\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 8\n",
      "(-0.13, 0.000858000858000918)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 9\n",
      "20.050606\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 8\n",
      "(0.0, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 9\n",
      "(0.97, 0.9978549978549979)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 10\n",
      "22.569935\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 9\n",
      "(-0.13, -1.1102230246251565e-16)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 10\n",
      "21.931526\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 9\n",
      "(0.0, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 10\n",
      "(0.96, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 11\n",
      "24.745445\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 10\n",
      "(-0.08999999999999997, 0.0004290004290005145)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 11\n",
      "24.015652\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 10\n",
      "(0.0, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 11\n",
      "(0.96, 0.9978549978549979)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 12\n",
      "26.939314\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 11\n",
      "(-0.13, -1.1102230246251565e-16)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 12\n",
      "26.102755\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 11\n",
      "(0.010000000000000009, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 12\n",
      "(0.96, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 13\n",
      "29.236748\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 12\n",
      "(-0.14, 1.1102230246251565e-16)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 13\n",
      "28.266085\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 12\n",
      "(0.0, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 13\n",
      "(0.97, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 14\n",
      "31.479925\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 13\n",
      "(-0.13, 0.0004290004290005145)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 14\n",
      "30.351995\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 13\n",
      "(0.0, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 14\n",
      "(0.96, 0.9978549978549979)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 15\n",
      "33.61597\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 14\n",
      "(-0.14, 0.00042900042900029245)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 15\n",
      "32.52082\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 14\n",
      "(0.0, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 15\n",
      "(0.97, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 16\n",
      "36.08714\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 15\n",
      "(-0.14, 0.0004290004290005145)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 16\n",
      "34.804897\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 15\n",
      "(0.0, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 16\n",
      "(0.96, 0.9978549978549979)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 17\n",
      "38.516216\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 16\n",
      "(-0.12, 0.00042900042900040347)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 17\n",
      "37.108173\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 16\n",
      "(0.0, 0.0)\n",
      "----------\n",
      "Performance of global model at timestep 17\n",
      "(0.97, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 18\n",
      "40.624905\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 17\n",
      "(-0.19999999999999996, 0.0004290004290005145)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 18\n",
      "39.266697\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 17\n",
      "(0.0, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 18\n",
      "(0.97, 0.9978549978549978)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 19\n",
      "43.314148\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 18\n",
      "(-0.14, 0.0004290004290005145)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 19\n",
      "41.754654\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 18\n",
      "(-0.010000000000000009, 1.1102230246251565e-16)\n",
      "----------\n",
      "Performance of global model at timestep 19\n",
      "(0.96, 0.9978549978549979)\n",
      "---------\n",
      "Alignment of client 0 with GRV at round 20\n",
      "45.76579\n",
      "---------\n",
      "Improvement of 0's update in isolation when applied to global model at round 19\n",
      "(-0.1499999999999999, 0.00042900042900040347)\n",
      "---------\n",
      "Alignment of client 1 with GRV at round 20\n",
      "44.055084\n",
      "---------\n",
      "Improvement of 1's update in isolation when applied to global model at round 19\n",
      "(0.0, 0.0)\n",
      "===============\n",
      "Client alignment matrix\n",
      "[[ 3.81867003  5.02602673  7.65638685 10.08057976 12.32711792 14.40653801\n",
      "  16.34801483 18.47105408 20.59863281 22.56993484 24.74544525 26.93931389\n",
      "  29.23674774 31.47992516 33.61597061 36.08713913 38.51621628 40.62490463\n",
      "  43.31414795 45.76578903]\n",
      " [ 4.32317257  5.23695755  7.75908947 10.10259151 12.23600769 14.1961813\n",
      "  16.09896469 18.09254265 20.05060577 21.93152618 24.0156517  26.10275459\n",
      "  28.26608467 30.35199547 32.52082062 34.80489731 37.10817337 39.26669693\n",
      "  41.75465393 44.05508423]]\n",
      "===============\n",
      "Client deletion matrix\n",
      "[[ 1.13769198e+00 -1.49571000e-01 -1.80000000e-01 -1.70000000e-01\n",
      "  -1.20000000e-01 -1.30000000e-01 -9.57099957e-03 -1.40000000e-01\n",
      "  -1.29141999e-01 -1.30000000e-01 -8.95709996e-02 -1.30000000e-01\n",
      "  -1.40000000e-01 -1.29571000e-01 -1.39571000e-01 -1.39571000e-01\n",
      "  -1.19571000e-01 -1.99571000e-01 -1.39571000e-01 -1.49571000e-01]\n",
      " [ 1.29726298e+00 -1.95709996e-02 -1.00000000e-02 -1.00000000e-02\n",
      "  -2.00000000e-02  1.00000000e-02  1.00000000e-02 -1.00000000e-02\n",
      "   0.00000000e+00  0.00000000e+00  1.11022302e-16  1.00000000e-02\n",
      "   1.11022302e-16  0.00000000e+00  0.00000000e+00  1.11022302e-16\n",
      "   0.00000000e+00  1.11022302e-16 -1.00000000e-02  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "client_alignment_matrix, client_deletion_matrix = contribution_calculation(model_hyperparams, global_states, client_state_dict, testing_dataset, 'Singular', scale_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC Metric Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<VirtualWorker id:worker1 #objects:2>: 0.25405786873676783,\n",
       " <VirtualWorker id:worker2 #objects:2>: 0.23782639378969656,\n",
       " <VirtualWorker id:worker3 #objects:2>: 0.25405786873676783,\n",
       " <VirtualWorker id:worker4 #objects:2>: 0.25405786873676783}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24981869263317313"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( np.sum(client_deletion_matrix[0])) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2075893178065091"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(client_deletion_matrix[1])) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036435247342763544"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(client_deletion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02973279789019421"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(client_deletion_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04313769679533288"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(client_deletion_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2257797122001648"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(client_alignment_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25909007418311314"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.sum(client_deletion_matrix[1])) ** 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.450413662422447"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.sum(client_deletion_matrix[0])) ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1700837621578986"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(((client_alignment_matrix - np.min(client_alignment_matrix)) / np.max(client_alignment_matrix))[0]) / len(client_alignment_matrix[0]) - ((np.sum(client_deletion_matrix[0])) ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.514166713553851"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(((client_alignment_matrix - np.min(client_alignment_matrix)) / np.max(client_alignment_matrix))[1]) / len(client_alignment_matrix[0]) - ((np.sum(client_deletion_matrix[1])) ** 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_contribution_matrix(mat):\n",
    "    return (mat - np.mean(mat)) / np.std(mat)\n",
    "\n",
    "def aggregate_contribution_matrices(arguments, align_mat, del_mat):\n",
    "#     align_mat = normalize_contribution_matrix(align_mat)\n",
    "#     del_mat = normalize_contribution_matrix(del_mat)\n",
    "    \n",
    "    contributions = defaultdict()\n",
    "    \n",
    "    for i in range(align_mat.shape[0]):\n",
    "        contributions[i] = np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds']\n",
    "#         print(np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds'])\n",
    "        print(np.sum(align_mat[i]) + np.sum(del_mat[i]) / arguments['rounds'])\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.94851156, -0.50448041, -0.60974259, -0.57514987, -0.4021863 ,\n",
       "        -0.43677902, -0.02018242, -0.47137173, -0.43381096, -0.43677902,\n",
       "        -0.29692413, -0.43677902, -0.47137173, -0.43529499, -0.4698877 ,\n",
       "        -0.4698877 , -0.40070227, -0.67744398, -0.4698877 , -0.50448041],\n",
       "       [ 4.50051095, -0.05477514, -0.02166645, -0.02166645, -0.05625916,\n",
       "         0.04751898,  0.04751898, -0.02166645,  0.01292626,  0.01292626,\n",
       "         0.01292626,  0.04751898,  0.01292626,  0.01292626,  0.01292626,\n",
       "         0.01292626,  0.01292626,  0.01292626, -0.02166645,  0.01292626]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_contribution_matrix(client_deletion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.60798579, -1.51056888, -1.29833536, -1.10273667, -0.92147225,\n",
       "        -0.75369194, -0.59704173, -0.42574195, -0.25407591, -0.09501922,\n",
       "         0.08051425,  0.25752897,  0.44289997,  0.62389322,  0.79624242,\n",
       "         0.99563139,  1.19162417,  1.36176603,  1.5787506 ,  1.77656398],\n",
       "       [-1.56727945, -1.49354969, -1.29004869, -1.10096063, -0.92882358,\n",
       "        -0.7706648 , -0.61713661, -0.45628257, -0.29829407, -0.14652993,\n",
       "         0.02163005,  0.19003026,  0.36458094,  0.53288497,  0.70787903,\n",
       "         0.89217229,  1.07801467,  1.25217753,  1.4529211 ,  1.63853387]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_contribution_matrix(client_alignment_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.55869753541214\n",
      "468.33683680835725\n"
     ]
    }
   ],
   "source": [
    "contributions = aggregate_contribution_matrices(model_hyperparams, client_alignment_matrix, client_deletion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 20])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2045, 20])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {0: 184.6675689593634,\n",
       "             1: 205.90674948397876,\n",
       "             2: 201.27492577070555,\n",
       "             3: 216.01619803535587,\n",
       "             4: 213.85393442505773})"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([479, 20])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: LogisticRegression(\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " 1: LogisticRegression(\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " 2: LogisticRegression(\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " ),\n",
       " 3: LogisticRegression(\n",
       "   (linear): Linear(in_features=20, out_features=1, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'linear.weight': [tensor([[ 0.0675,  0.0123, -0.0878, -0.0320, -0.0404,  0.0702, -0.2106, -0.0237,\n",
       "             0.2144, -0.0160,  0.1440, -0.0417,  0.1804,  0.1131,  0.0626,  0.1555,\n",
       "             0.1011,  0.0177, -0.0217,  0.2229]]),\n",
       "   tensor([[ 0.0710,  0.0338, -0.0854, -0.0220, -0.0225,  0.0676, -0.1689, -0.0601,\n",
       "             0.2198, -0.0261,  0.1218, -0.0033,  0.1829,  0.0892,  0.0941,  0.1658,\n",
       "             0.0940,  0.0277,  0.0075,  0.2114]])],\n",
       "  'linear.bias': [tensor([-0.1748]), tensor([-0.1602])]},\n",
       " 2: {'linear.weight': [tensor([[ 0.0673,  0.0135, -0.0880, -0.0298, -0.0405,  0.0727, -0.2098, -0.0222,\n",
       "             0.2144, -0.0149,  0.1440, -0.0409,  0.1790,  0.1131,  0.0621,  0.1557,\n",
       "             0.1002,  0.0174, -0.0221,  0.2253]]),\n",
       "   tensor([[ 0.0726,  0.0356, -0.0852, -0.0209, -0.0219,  0.0653, -0.1697, -0.0592,\n",
       "             0.2204, -0.0238,  0.1242, -0.0053,  0.1822,  0.0901,  0.0939,  0.1654,\n",
       "             0.0924,  0.0275,  0.0074,  0.2126]])],\n",
       "  'linear.bias': [tensor([-0.1729]), tensor([-0.1582])]},\n",
       " 3: {'linear.weight': [tensor([[ 0.0672,  0.0127, -0.0881, -0.0315, -0.0409,  0.0728, -0.2111, -0.0231,\n",
       "             0.2159, -0.0149,  0.1444, -0.0412,  0.1789,  0.1117,  0.0631,  0.1549,\n",
       "             0.1005,  0.0179, -0.0210,  0.2225]]),\n",
       "   tensor([[ 0.0717,  0.0339, -0.0854, -0.0211, -0.0226,  0.0621, -0.1710, -0.0587,\n",
       "             0.2179, -0.0257,  0.1230, -0.0040,  0.1826,  0.0910,  0.0932,  0.1654,\n",
       "             0.0937,  0.0275,  0.0061,  0.2114]])],\n",
       "  'linear.bias': [tensor([-0.1749]), tensor([-0.1609])]}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1710,  0.1856, -0.0524,  0.2054, -0.0490,  0.0451, -0.1089,  0.1313,\n",
       "          0.1971, -0.1640,  0.1944,  0.0419,  0.1652,  0.0303,  0.1078, -0.0316,\n",
       "          0.1724,  0.0331, -0.1044,  0.0570]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_states[0].state_dict()['linear.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1389,  0.0466, -0.1736, -0.0526, -0.0635,  0.1349, -0.3821, -0.0818,\n",
       "          0.4338, -0.0406,  0.2675, -0.0452,  0.3614,  0.2028,  0.1563,  0.3203,\n",
       "          0.1941,  0.0454, -0.0149,  0.4339]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_states[3].state_dict()['linear.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'linear.weight': [tensor([[ 0.2639, -0.1727,  0.0589,  0.0170, -0.1685, -0.0229,  0.0598,  0.4092,\n",
       "            -0.0129,  0.0305,  0.0777,  0.0037,  0.0768, -0.0218,  0.0458, -0.0179,\n",
       "             0.2290, -0.1439,  0.0872, -0.0690]]),\n",
       "   tensor([[ 0.2665, -0.1948, -0.0414, -0.0178, -0.1609,  0.0041,  0.0192,  0.3789,\n",
       "             0.0055,  0.0171,  0.0241,  0.0216,  0.0726, -0.0494,  0.0140, -0.0183,\n",
       "             0.2087, -0.1459, -0.0615, -0.0935]])],\n",
       "  'linear.bias': [tensor([-0.0484]), tensor([0.0329])]},\n",
       " 2: {'linear.weight': [tensor([[ 0.2614, -0.1653,  0.0704,  0.0209, -0.1635, -0.0239,  0.0569,  0.4063,\n",
       "            -0.0140,  0.0340,  0.0716, -0.0045,  0.0659, -0.0201,  0.0451, -0.0138,\n",
       "             0.2262, -0.1293,  0.0772, -0.0756]]),\n",
       "   tensor([[ 0.2630, -0.1939, -0.0406, -0.0179, -0.1616,  0.0090,  0.0101,  0.3768,\n",
       "             0.0100, -0.0005,  0.0196,  0.0136,  0.0787, -0.0457,  0.0158, -0.0216,\n",
       "             0.2061, -0.1513, -0.0692, -0.0842]])],\n",
       "  'linear.bias': [tensor([-0.0420]), tensor([0.0252])]},\n",
       " 3: {'linear.weight': [tensor([[ 0.2620, -0.1602,  0.0641,  0.0141, -0.1673, -0.0242,  0.0544,  0.4054,\n",
       "            -0.0019,  0.0312,  0.0673, -0.0070,  0.0696, -0.0272,  0.0427, -0.0299,\n",
       "             0.2310, -0.1258,  0.0749, -0.0797]]),\n",
       "   tensor([[ 0.2661, -0.1948, -0.0458, -0.0182, -0.1562,  0.0039,  0.0181,  0.3646,\n",
       "             0.0076, -0.0015,  0.0129,  0.0160,  0.0761, -0.0559,  0.0213, -0.0211,\n",
       "             0.2081, -0.1507, -0.0586, -0.0953]])],\n",
       "  'linear.bias': [tensor([-0.0441]), tensor([0.0345])]},\n",
       " 4: {'linear.weight': [tensor([[ 0.2659, -0.1756,  0.0635,  0.0136, -0.1647, -0.0188,  0.0642,  0.4070,\n",
       "            -0.0144,  0.0420,  0.0804, -0.0021,  0.0703, -0.0233,  0.0516, -0.0208,\n",
       "             0.2278, -0.1412,  0.0769, -0.0756]]),\n",
       "   tensor([[ 0.2641, -0.1964, -0.0456, -0.0141, -0.1582,  0.0064,  0.0160,  0.3705,\n",
       "             0.0117,  0.0048,  0.0122,  0.0181,  0.0793, -0.0479,  0.0175, -0.0141,\n",
       "             0.2060, -0.1557, -0.0589, -0.0853]])],\n",
       "  'linear.bias': [tensor([-0.0459]), tensor([0.0358])]},\n",
       " 5: {'linear.weight': [tensor([[ 0.2632, -0.1629,  0.0638,  0.0184, -0.1653, -0.0167,  0.0525,  0.4057,\n",
       "            -0.0089,  0.0350,  0.0749,  0.0014,  0.0734, -0.0218,  0.0441, -0.0216,\n",
       "             0.2340, -0.1366,  0.0777, -0.0720]]),\n",
       "   tensor([[ 0.2631, -0.1943, -0.0438, -0.0206, -0.1566, -0.0014,  0.0225,  0.3670,\n",
       "             0.0113, -0.0018,  0.0207,  0.0102,  0.0802, -0.0478,  0.0130, -0.0180,\n",
       "             0.2063, -0.1563, -0.0624, -0.0841]])],\n",
       "  'linear.bias': [tensor([-0.0509]), tensor([0.0276])]}}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:worker1 #objects:8>\n",
      "(Wrapper)>[PointerTensor | me:76483481482 -> crypto_provider:71511273569]::data\n",
      "<VirtualWorker id:worker2 #objects:8>\n",
      "(Wrapper)>[PointerTensor | me:54640959924 -> crypto_provider:5913270987]::data\n"
     ]
    }
   ],
   "source": [
    "for worker, dict in client_states[1].items():\n",
    "    print(worker)\n",
    "    print(dict.state_dict()['linear.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Before getting, model parameters:\n",
      " [Parameter containing:\n",
      "Parameter>[PointerTensor | me:36712146433 -> crypto_provider:38161928650], Parameter containing:\n",
      "Parameter>[PointerTensor | me:53766344238 -> crypto_provider:52911434509]]\n",
      "Before getting, model location:\n",
      " [<VirtualWorker id:crypto_provider #objects:84>, <VirtualWorker id:crypto_provider #objects:84>]\n",
      "Before getting, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296, 38161928650, 52911434509]\n",
      "Current worker: <VirtualWorker id:worker1 #objects:4>\n",
      "Before sending, model parameters:\n",
      " [Parameter containing:\n",
      "tensor([[ 0.1397,  0.0466, -0.1732, -0.0530, -0.0632,  0.1373, -0.3795, -0.0828,\n",
      "          0.4354, -0.0397,  0.2659, -0.0446,  0.3606,  0.2023,  0.1575,  0.3202,\n",
      "          0.1943,  0.0449, -0.0140,  0.4341]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3338], requires_grad=True)]\n",
      "Before sending, model location:\n",
      " [None, None]\n",
      "Before sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296]\n",
      "Before sending, worker objects:\n",
      " dict_keys([63999975985, 84285192018, 3210024390, 21098284358])\n",
      "Before sending, are model parameters in worker? False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After sending, model parameters:\n",
      " [Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:71211976139 -> worker1:53438888134], Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:39905936064 -> worker1:81508625698]]\n",
      "After sending, model location:\n",
      " [<VirtualWorker id:worker1 #objects:6>, <VirtualWorker id:worker1 #objects:6>]\n",
      "After sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296]\n",
      "After sending, worker objects:\n",
      " dict_keys([63999975985, 84285192018, 3210024390, 21098284358, 53438888134, 81508625698])\n",
      "After sending, are model parameters in worker? False\n",
      "(Wrapper)>[PointerTensor | me:50145786224 -> worker1:77820585669] (Wrapper)>[PointerTensor | me:14210659634 -> worker1:51214479127]\n",
      "Validation scores: (0.7843137254901961, 0.7423659141328462)\n",
      "Before getting, model parameters:\n",
      " [Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:44130832361 -> crypto_provider:53438888134], Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:80657783293 -> crypto_provider:81508625698]]\n",
      "Before getting, model location:\n",
      " [<VirtualWorker id:crypto_provider #objects:84>, <VirtualWorker id:crypto_provider #objects:84>]\n",
      "Before getting, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296, 53438888134, 81508625698]\n",
      "Current worker: <VirtualWorker id:worker2 #objects:4>\n",
      "Before sending, model parameters:\n",
      " [Parameter containing:\n",
      "tensor([[ 0.1397,  0.0466, -0.1732, -0.0530, -0.0632,  0.1373, -0.3795, -0.0828,\n",
      "          0.4354, -0.0397,  0.2659, -0.0446,  0.3606,  0.2023,  0.1575,  0.3202,\n",
      "          0.1943,  0.0449, -0.0140,  0.4341]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3338], requires_grad=True)]\n",
      "Before sending, model location:\n",
      " [None, None]\n",
      "Before sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296]\n",
      "Before sending, worker objects:\n",
      " dict_keys([31467788079, 91454758974, 24662750769, 58458599022])\n",
      "Before sending, are model parameters in worker? False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After sending, model parameters:\n",
      " [Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:59581852667 -> worker2:53438888134], Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:24774858368 -> worker2:81508625698]]\n",
      "After sending, model location:\n",
      " [<VirtualWorker id:worker2 #objects:6>, <VirtualWorker id:worker2 #objects:6>]\n",
      "After sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296]\n",
      "After sending, worker objects:\n",
      " dict_keys([31467788079, 91454758974, 24662750769, 58458599022, 53438888134, 81508625698])\n",
      "After sending, are model parameters in worker? False\n",
      "(Wrapper)>[PointerTensor | me:24686176546 -> worker2:85402627087] (Wrapper)>[PointerTensor | me:95122638758 -> worker2:81651396872]\n",
      "Validation scores: (0.7607843137254902, 0.7330765344784036)\n"
     ]
    }
   ],
   "source": [
    "def perform_FL_testing(arguments, dataset, model): \n",
    "    \"\"\" Obtains predictions given a validation/test dataset upon \n",
    "        a specified trained global model.\n",
    "        \n",
    "    Args:\n",
    "        arguments (Arguments): Parameters defining current experiment\n",
    "        dataset (tuple(th.Tensor)): A validation/test dataset\n",
    "        model   (nn.Module): Trained global model\n",
    "    Returns:\n",
    "        accuracy score (float)\n",
    "        roc_auc score  (float)\n",
    "    \"\"\"\n",
    "    # Archive model's location (Default: TTP)\n",
    "    model_origin = model.location\n",
    "\n",
    "    print(\"Before getting, model parameters:\\n\", list(model.parameters()))\n",
    "    print(\"Before getting, model location:\\n\", [p.location for p in list(model.parameters())])\n",
    "    print(\"Before getting, objects at TTP:\\n\", list(crypto_provider._objects.keys()))\n",
    "    \n",
    "    # Trace the location of the dataset to be evaluated\n",
    "    curr_worker = dataset.location\n",
    "    print(\"Current worker:\", curr_worker)\n",
    "    \n",
    "    # Retrieve model from TTP\n",
    "    model = model.get()\n",
    "    \n",
    "    print(\"Before sending, model parameters:\\n\", list(model.parameters()))\n",
    "    print(\"Before sending, model location:\\n\", [p.location for p in list(model.parameters())])\n",
    "    print(\"Before sending, objects at TTP:\\n\", list(crypto_provider._objects.keys()))\n",
    "    print(\"Before sending, worker objects:\\n\", curr_worker._objects.keys())\n",
    "    print(\"Before sending, are model parameters in worker?\", \n",
    "          all([p.location in curr_worker._objects.keys()\n",
    "           for p in list(model.parameters())]))\n",
    "    \n",
    "    model = model.send(curr_worker)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"After sending, model parameters:\\n\", list(model.parameters()))\n",
    "    print(\"After sending, model location:\\n\", [p.location for p in list(model.parameters())])\n",
    "    print(\"After sending, objects at TTP:\\n\", list(crypto_provider._objects.keys()))\n",
    "    print(\"After sending, worker objects:\\n\", curr_worker._objects.keys())\n",
    "    print(\"After sending, are model parameters in worker?\", \n",
    "          all([p.location in curr_worker._objects.keys()\n",
    "           for p in list(model.parameters())]))\n",
    "    \n",
    "    X_test = dataset.data.float()\n",
    "    y_test = dataset.targets.float()\n",
    "    print(X_test, y_test)\n",
    "    \n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        \n",
    "        if arguments.is_condensed:\n",
    "            predictions = (model(X_test) > 0.5).float()\n",
    "            \n",
    "        else:\n",
    "            # Find best predicted class label representative of sample\n",
    "            _, predicted_labels = model(X_test).max(axis=1)\n",
    "            \n",
    "            # One-hot encode predicted labels\n",
    "            predictions = th.FloatTensor(y_test.shape)\n",
    "            predictions.zero_()\n",
    "            predictions.scatter_(1, predicted_labels.view(-1,1), 1)\n",
    "\n",
    "\n",
    "        # Retrieve predictions\n",
    "        predictions = predictions.get()\n",
    "        \n",
    "        # Retrieve truth labels\n",
    "        y_test = y_test.get()\n",
    "        \n",
    "        # Calculate accuracy of predictions\n",
    "        accuracy = accuracy_score(y_test.numpy(), predictions.numpy())\n",
    "        \n",
    "        # Calculate ROC-AUC for each label\n",
    "        roc = roc_auc_score(y_test.numpy(), predictions.numpy())\n",
    "\n",
    "        #############\n",
    "        # Quick Fix #\n",
    "        #############\n",
    "        # y_test is located at some remote worker. Accuracy & ROC-AUC computation via\n",
    "        # numpy raises errors because Numpy is currently not supported. Manual retrieval\n",
    "        # and sending first, manual implementation to be done later.\n",
    "        y_test = y_test.send(curr_worker)\n",
    "        \n",
    "        # Retrieve & send model back to whence it came!\n",
    "        model.get()\n",
    "        model = model.send(model_origin)\n",
    " \n",
    "    return accuracy, roc\n",
    "\n",
    "\n",
    "test_model = copy.deepcopy(binary_sgd_model)\n",
    "\n",
    "print(\"=\"*100)\n",
    "for worker, dataset in validation_pointers.items():\n",
    "\n",
    "    print(\"Validation scores:\", perform_FL_testing(best_binary_sgd_args, dataset, test_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before getting, model parameters:\n",
      " [Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:32719714031 -> crypto_provider:53438888134], Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:70939440670 -> crypto_provider:81508625698]]\n",
      "Before getting, model location:\n",
      " [<VirtualWorker id:crypto_provider #objects:84>, <VirtualWorker id:crypto_provider #objects:84>]\n",
      "Before getting, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296, 53438888134, 81508625698]\n",
      "Current worker: <VirtualWorker id:crypto_provider #objects:84>\n",
      "Before sending, model parameters:\n",
      " [Parameter containing:\n",
      "tensor([[ 0.1397,  0.0466, -0.1732, -0.0530, -0.0632,  0.1373, -0.3795, -0.0828,\n",
      "          0.4354, -0.0397,  0.2659, -0.0446,  0.3606,  0.2023,  0.1575,  0.3202,\n",
      "          0.1943,  0.0449, -0.0140,  0.4341]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3338], requires_grad=True)]\n",
      "Before sending, model location:\n",
      " [None, None]\n",
      "Before sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296]\n",
      "Before sending, worker objects:\n",
      " dict_keys([21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296])\n",
      "Before sending, are model parameters in worker? False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After sending, model parameters:\n",
      " [Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:69984526734 -> crypto_provider:53438888134], Parameter containing:\n",
      "(Wrapper)>[PointerTensor | me:91308746184 -> crypto_provider:81508625698]]\n",
      "After sending, model location:\n",
      " [<VirtualWorker id:crypto_provider #objects:84>, <VirtualWorker id:crypto_provider #objects:84>]\n",
      "After sending, objects at TTP:\n",
      " [21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296, 53438888134, 81508625698]\n",
      "After sending, worker objects:\n",
      " dict_keys([21932075915, 9090941217, 79944881906, 89279046959, 89850986629, 57346283611, 73689102421, 82730040201, 79385862149, 73258156428, 44969379986, 4382029053, 36007568665, 46463523158, 32503582200, 27214527800, 44473247553, 39417226305, 18137575301, 33877527535, 20308527827, 20833382737, 47173841097, 67935573675, 31014787181, 57698471772, 31036289465, 53025945477, 74383302444, 79078125948, 71985433180, 29270494094, 94407237437, 43010196674, 15604950872, 73754126399, 34795441738, 30330599691, 37350792097, 48319690060, 91752207350, 59816065840, 7312995878, 25309879949, 69841944925, 26533179963, 53047552083, 54199063962, 84299236018, 56102117224, 73075100890, 17536064666, 2482631677, 12423015975, 22857114530, 35233594670, 3351325830, 46778026967, 41175173769, 19470781002, 87643614385, 78499059248, 10206587004, 34856568341, 92329733120, 74258215376, 59761543923, 53522115175, 17155269344, 66830851070, 71379897226, 59801753789, 17200133758, 95250689892, 18308883418, 29300206517, 13291087870, 71335024180, 62796946818, 99987532642, 17133559125, 95889305296, 53438888134, 81508625698])\n",
      "After sending, are model parameters in worker? False\n",
      "(Wrapper)>[PointerTensor | me:42372669819 -> crypto_provider:56730270812] (Wrapper)>[PointerTensor | me:57355077146 -> crypto_provider:53848287163]\n",
      "Accuracy scores: (0.8023255813953488, 0.782405498281787)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy scores:\", perform_FL_testing(best_binary_sgd_args, testing_pointer, test_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " (Wrapper)>[PointerTensor | me:36712146433 -> crypto_provider:38161928650],\n",
       " Parameter containing:\n",
       " (Wrapper)>[PointerTensor | me:53766344238 -> crypto_provider:52911434509]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that model parameters are hidden\n",
    "list(binary_sgd_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ((Wrapper)>[PointerTensor | me:98952335063 -> worker1:63999975985],\n",
       "  (Wrapper)>[PointerTensor | me:68461425098 -> worker1:84285192018]),\n",
       " 1: ((Wrapper)>[PointerTensor | me:87636762647 -> worker2:31467788079],\n",
       "  (Wrapper)>[PointerTensor | me:39563190106 -> worker2:91454758974])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that training datasets are still hidden from Me (i.e. client)\n",
    "training_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ((Wrapper)>[PointerTensor | me:22904807196 -> worker1:3210024390],\n",
       "  (Wrapper)>[PointerTensor | me:26774087597 -> worker1:21098284358]),\n",
       " 1: ((Wrapper)>[PointerTensor | me:26488016649 -> worker2:24662750769],\n",
       "  (Wrapper)>[PointerTensor | me:26274160852 -> worker2:58458599022])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that validation datasets are still hidden from Me (i.e. client)\n",
    "validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Wrapper)>[PointerTensor | me:39940050514 -> crypto_provider:21932075915],\n",
       " (Wrapper)>[PointerTensor | me:50952629367 -> crypto_provider:9090941217])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that test_dataset is hidden from me (i.e. client)\n",
    "testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Model privacy is preserved!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Conclusion**\n",
    "    \n",
    "The manual computation & aggregation of model weights per round seems to give rise to an interesting phenomenom; IID validation performance seem to perform poorly, but the generated model appears to be better able to generalise to testing data, as seen from the 4% increase in accuracy (i.e. ~76% to ~80%) and roc-auc (i.e. ~73% to 78%).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to do\n",
    "# 1) test data from client must be SMPC-ed\n",
    "# 2) Model weights & biases must be SMPC-ed (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Deprecated #\n",
    "##############\n",
    "\n",
    "\"\"\"\n",
    "from pprint import pprint\n",
    "for data, labels in sy.FederatedDataLoader(sy.FederatedDataset(training_pointers.values()), \n",
    "                                           batch_size=100,\n",
    "                                           iter_per_worker=False):\n",
    "    pprint(data)\n",
    "    \n",
    "    \n",
    "from pprint import pprint\n",
    "for a in sy.FederatedDataLoader(sy.FederatedDataset(training_pointers.values()), \n",
    "                                           batch_size=100,\n",
    "                                           iter_per_worker=True):\n",
    "    pprint(a)\n",
    "\n",
    "len(sy.FederatedDataLoader(sy.FederatedDataset(training_pointers.values()), \n",
    "                                           batch_size=100,\n",
    "                                           iter_per_worker=True))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
