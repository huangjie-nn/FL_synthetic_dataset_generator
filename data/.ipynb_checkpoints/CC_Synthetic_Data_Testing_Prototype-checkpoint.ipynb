{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Import all required modules, hook PySyft, declare functions for simulating FL environment (Star Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n",
    "\n",
    "####################\n",
    "# Required Modules #\n",
    "####################\n",
    "\n",
    "# Generic\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Libs\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import sklearn.datasets as skld\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from tqdm.notebook import trange\n",
    "from IPython.display import display\n",
    "\n",
    "# Custom\n",
    "from src.datapipeline import Preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proportions = [[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                   [0.3, 0.3, 0.13, 0.13, 0.13], \n",
    "                   [0.4, 0.3, 0.2, 0.05, 0.05],\n",
    "                   [0.7, 0.2, 0.03, 0.03, 0.03]]\n",
    "\n",
    "DGPs = [[(0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1)],\n",
    "       [(0, 0.2), (0, 0.2), (0, 0.1), (0, 0.1), (0, 0.1)], \n",
    "       [(0, 0.3), (0, 0.2), (0, 0.1), (0, 0.1), (0, 0.1)],\n",
    "       [(0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1)]]\n",
    "\n",
    "distribution_of_labels = [[(0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)],\n",
    "                           [(0.7, 0.3), (0.6, 0.4), (0.6, 0.4), (0.5, 0.5), (0.5, 0.5)], \n",
    "                           [(0.85, 0.15), (0.8, 0.2), (0.7, 0.3), (0.6, 0.4), (0.5, 0.5)],\n",
    "                           [(1, 0), (0, 1), (1, 0), (0, 1), (0, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "NUM_DIM = 20\n",
    "class SyntheticDataset:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=2,\n",
    "            seed=round(np.random.random() * 100),\n",
    "            num_dim=NUM_DIM,\n",
    "            prob_clusters=[0.5, 0.5]):\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_dim = num_dim\n",
    "        self.num_clusters = len(prob_clusters)\n",
    "        self.prob_clusters = prob_clusters\n",
    "\n",
    "        self.side_info_dim = self.num_clusters\n",
    "\n",
    "        self.Q = np.random.normal(\n",
    "            loc=0.0, scale=1.0, size=(self.num_dim + 1, self.num_classes, self.side_info_dim))\n",
    "\n",
    "        self.Sigma = np.zeros((self.num_dim, self.num_dim))\n",
    "        for i in range(self.num_dim):\n",
    "            self.Sigma[i, i] = (i + 1)**(-1.2)\n",
    "\n",
    "        self.means = self._generate_clusters()\n",
    "\n",
    "    def get_task(self, num_samples):\n",
    "        cluster_idx = np.random.choice(\n",
    "            range(self.num_clusters), size=None, replace=True, p=self.prob_clusters)\n",
    "        new_task = self._generate_task(self.means[cluster_idx], cluster_idx, num_samples)\n",
    "        return new_task\n",
    "\n",
    "    def _generate_clusters(self):\n",
    "        means = []\n",
    "        for i in range(self.num_clusters):\n",
    "            loc = np.random.normal(loc=0, scale=1., size=None)\n",
    "            mu = np.random.normal(loc=loc, scale=1., size=self.side_info_dim)\n",
    "            means.append(mu)\n",
    "        return means\n",
    "\n",
    "    def _generate_x(self, num_samples):\n",
    "        B = np.random.normal(loc=0.0, scale=1.0, size=None)\n",
    "        loc = np.random.normal(loc=B, scale=1.0, size=self.num_dim)\n",
    "\n",
    "        samples = np.ones((num_samples, self.num_dim + 1))\n",
    "        samples[:, 1:] = np.random.multivariate_normal(\n",
    "            mean=loc, cov=self.Sigma, size=num_samples)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _generate_y(self, x, cluster_mean):\n",
    "        model_info = np.random.normal(loc=cluster_mean, scale=0.1, size=cluster_mean.shape)\n",
    "        w = np.matmul(self.Q, model_info)\n",
    "        \n",
    "        num_samples = x.shape[0]\n",
    "        prob = softmax(np.matmul(x, w) + np.random.normal(loc=0., scale=0.1, size=(num_samples, self.num_classes)), axis=1)\n",
    "                \n",
    "        y = np.argmax(prob, axis=1)\n",
    "        return y, w, model_info\n",
    "\n",
    "    def _generate_task(self, cluster_mean, cluster_id, num_samples):\n",
    "        x = self._generate_x(num_samples)\n",
    "        y, w, model_info = self._generate_y(x, cluster_mean)\n",
    "\n",
    "        # now that we have y, we can remove the bias coeff\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        return {'x': x, 'y': y, 'w': w, 'model_info': model_info, 'cluster': cluster_id}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([[-2.19375009,  1.32144189,  0.48307649, ..., -1.53719885,\n",
       "         -0.17183379, -1.45830464],\n",
       "        [-1.04096292,  0.74812346,  0.69908505, ..., -2.05817883,\n",
       "         -0.22098152, -1.20791264],\n",
       "        [-2.54963164,  0.06346844,  0.32003966, ..., -1.74613894,\n",
       "         -0.17518618, -0.96918059],\n",
       "        ...,\n",
       "        [-2.22060479, -0.04840877,  0.80082354, ..., -1.65613586,\n",
       "         -0.28828986, -1.2509536 ],\n",
       "        [-2.64532884, -0.63004091,  0.58311658, ..., -1.6780055 ,\n",
       "         -0.34086216, -1.04046092],\n",
       "        [-2.19337052, -0.38636502,  0.26911878, ..., -2.0223011 ,\n",
       "         -0.07011181, -1.17605659]]),\n",
       " 'y': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int64),\n",
       " 'w': array([[ 0.78544453, -0.73449339],\n",
       "        [-2.67236588,  1.3390077 ],\n",
       "        [ 0.50969413, -0.82637798],\n",
       "        [-0.20378509, -0.28751203],\n",
       "        [ 0.6000608 , -0.6303216 ],\n",
       "        [-0.58684981, -1.52834153],\n",
       "        [ 0.8534385 , -0.45110333],\n",
       "        [-0.07418779,  0.81547795],\n",
       "        [ 1.16044361,  0.47925992],\n",
       "        [-3.36214127, -1.04674814],\n",
       "        [-2.17858957, -1.32255368],\n",
       "        [ 0.05125086,  1.14513261],\n",
       "        [ 2.6999158 ,  2.13164394],\n",
       "        [ 0.12153482, -0.02878267],\n",
       "        [ 0.73135193, -0.62699871],\n",
       "        [ 1.09329718,  0.18374356],\n",
       "        [-1.47242538, -0.51579972],\n",
       "        [ 1.64305813, -0.89911995],\n",
       "        [-0.08485731,  1.20253805],\n",
       "        [-1.35765366, -2.11277582],\n",
       "        [-0.53183025,  0.81159273]]),\n",
       " 'model_info': array([0.67413772, 0.98108789]),\n",
       " 'cluster': 1}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_gen = SyntheticDataset()\n",
    "synth_gen.get_task(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secret_share(tensor, workers, crypto_provider, precision_fractional):\n",
    "    \"\"\" Transform to fixed precision and secret share a tensor \n",
    "    \n",
    "    Args:\n",
    "        tensor             (PointerTensor): Pointer to be shared\n",
    "        workers   (list(sy.VirtualWorker)): Involved workers of the grid\n",
    "        crypto_provider (sy.VirtualWorker): Arbiter (i.e. TTP) of the grid\n",
    "        precision_fractional (int): Precision for casting integer ring arithimetic\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tensor\n",
    "        .fix_precision(precision_fractional=precision_fractional)\n",
    "        .share(\n",
    "            *workers, \n",
    "            crypto_provider=crypto_provider, \n",
    "            requires_grad=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "def setup_FL_env(training_datasets, validation_datasets, \n",
    "                 testing_dataset, is_shared=False):\n",
    "    \"\"\" Sets up a basic federated learning environment using virtual workers,\n",
    "        with a allocated arbiter (i.e. TTP) to faciliate in model development\n",
    "        & utilisation, and deploys datasets to their respective workers\n",
    "        \n",
    "    Args:\n",
    "\n",
    "        training_datasets   (dict(tuple(th.Tensor))): Datasets to be used for training\n",
    "        validation_datasets (dict(tuple(th.Tensor))): Datasets to be used for validation\n",
    "        testing_dataset           (tuple(th.Tensor)): Datasets to be used for testing\n",
    "        is_shared (bool): Toggles if SMPC encryption protocols are active\n",
    "    Returns:\n",
    "        training_pointers  (dict(sy.BaseDataset))\n",
    "        validation_pointer (dict(sy.BaseDataset))\n",
    "        testing_pointer    (sy.BaseDataset)\n",
    "        workers            (list(sy.VirtualWorker))\n",
    "        crypto_provider    (sy.VirtualWorker)\n",
    "    \"\"\"\n",
    "    # Simulate FL computation amongst K worker nodes, \n",
    "    # where K is the no. of datasets to be federated\n",
    "    workers = connect_to_workers(n_workers=len(training_datasets))\n",
    "    \n",
    "    # Allow for 1 exchanger/Arbiter (i.e. TTP)\n",
    "    crypto_provider = connect_to_crypto_provider()\n",
    "    crypto_provider.clear_objects()\n",
    "    \n",
    "    assert (len(crypto_provider._objects) == 0)\n",
    "    \n",
    "    # Send training & validation datasets to their respective workers\n",
    "    training_pointers = {}\n",
    "    validation_pointers = {}\n",
    "    for w_idx in range(len(workers)):\n",
    "\n",
    "        # Retrieve & prepare worker for receiving dataset\n",
    "        curr_worker = workers[w_idx]\n",
    "        curr_worker.clear_objects()\n",
    "\n",
    "        assert (len(curr_worker._objects) == 0)\n",
    "\n",
    "        train_data = training_datasets[w_idx]\n",
    "        validation_data = validation_datasets[w_idx]\n",
    "        \n",
    "        # Cast dataset into a Tensor & send it to the relevant worker\n",
    "        train_pointer = sy.BaseDataset(*train_data).send(curr_worker)\n",
    "        validation_pointer = sy.BaseDataset(*validation_data).send(curr_worker)\n",
    "        \n",
    "        # Store data pointers for subsequent reference\n",
    "        training_pointers[curr_worker] = train_pointer\n",
    "        validation_pointers[curr_worker] = validation_pointer\n",
    "    \n",
    "    # 'Me' serves as the client -> test pointer stays with me, but is shared via SMPC\n",
    "    testing_pointer = sy.BaseDataset(*testing_dataset).send(crypto_provider)\n",
    "    \n",
    "    return training_pointers, validation_pointers, testing_pointer, workers, crypto_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_FL_batches(model_hyperparams, train_pointers, validation_pointers, test_pointer): \n",
    "    \"\"\" Supplementary function to convert initialised datasets into their\n",
    "        SGD compatible dataloaders in the context of PySyft's federated learning\n",
    "        (NOTE: This is based on the assumption that querying database size does\n",
    "               not break FL abstraction (i.e. not willing to share quantity))\n",
    "    Args:\n",
    "        model_hyperparams                      (model_hyperparams): Parameters defining current experiment\n",
    "        train_pointers      (dict(sy.BaseDataset)): Distributed datasets for training\n",
    "        validation_pointers (dict(sy.BaseDataset)): Distributed datasets for model calibration\n",
    "        test_pointer              (sy.BaseDataset): Distributed dataset for verifying performance\n",
    "    Returns:\n",
    "        train_loaders     (sy.FederatedDataLoader)\n",
    "        validation_loader (sy.FederatedDataLoader)\n",
    "        test_loader       (sy.FederatedDataLoader)\n",
    "    \"\"\"\n",
    "    \n",
    "    def construct_FL_loader(data_pointers, **kwargs):\n",
    "        \"\"\" Cast paired data & labels into configured tensor dataloaders\n",
    "        Args:\n",
    "            dataset (list(sy.BaseDataset)): A tuple of X features & y labels\n",
    "            kwargs: Additional parameters to configure PyTorch's Dataloader\n",
    "        Returns:\n",
    "            Configured dataloader (th.utils.data.DataLoader)\n",
    "        \"\"\"\n",
    "        federated_dataset = sy.FederatedDataset(data_pointer)\n",
    "        \n",
    "#         print(federated_dataset)\n",
    "        \n",
    "        federated_data_loader = sy.FederatedDataLoader(\n",
    "            federated_dataset, \n",
    "            batch_size=(\n",
    "                model_hyperparams['batch_size']\n",
    "                if model_hyperparams['batch_size'] \n",
    "                else len(federated_dataset)\n",
    "            ), \n",
    "            shuffle=True,\n",
    "            iter_per_worker=True, # for subsequent parallelization\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return federated_data_loader\n",
    "        \n",
    "        \n",
    "    # Load training pointers into a configured federated dataloader\n",
    "    train_loader = construct_FL_loader(train_pointers.values())\n",
    "    \n",
    "    # Load validation pointer into a configured federated dataloader\n",
    "    validation_loader = construct_FL_loader(validation_pointers.values())\n",
    "    \n",
    "    # Load testing dataset into a configured federated dataloader\n",
    "    test_loader = construct_FL_loader([test_pointer])\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_pointers, \n",
    " validation_pointers, \n",
    " testing_pointer, \n",
    " workers, \n",
    " crypto_provider) = setup_FL_env(\n",
    "    training_datasets,\n",
    "    validation_datasets, \n",
    "    testing_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 20])\n",
      "torch.Size([9, 20])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(batch[0].shape)\n",
    "#     for worker, (data, labels) in batch.items():\n",
    "# #         print(\"========\")\n",
    "# #         print(batch_idx, batch)\n",
    "#         print(worker)\n",
    "#         print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:worker1 #objects:2>, <VirtualWorker id:worker2 #objects:4>]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_client_key = list(training_pointers.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {one_client_key:training_pointers[one_client_key]}\n",
    "val = {one_client_key:validation_pointers[one_client_key]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<VirtualWorker id:worker1 #objects:2>: <syft.frameworks.torch.fl.dataloader.FederatedDataLoader object at 0x000001B868743748>, <VirtualWorker id:worker2 #objects:2>: <syft.frameworks.torch.fl.dataloader.FederatedDataLoader object at 0x000001B854829EC8>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainloaders = {}\n",
    "for worker_id in list(training_pointers.keys()):\n",
    "    train = {worker_id:training_pointers[worker_id]}\n",
    "    val = {worker_id:validation_pointers[worker_id]}\n",
    "    #============\n",
    "    # Convert training datasets into syft dataloaders. \n",
    "    train_loader, validation_loader, test_loader = convert_to_FL_batches(\n",
    "        binary_model_hyperparams,\n",
    "        train, \n",
    "        val, \n",
    "        testing_pointer\n",
    "    )\n",
    "    trainloaders.update({worker_id : train_loader})\n",
    "    \n",
    "print(trainloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 1])\n",
      "torch.Size([9, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([45, 1])\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "for key in trainloaders: \n",
    "    for batch_idx, batch in enumerate(trainloaders[key]):\n",
    "        print(batch[1].shape)\n",
    "#         for worker, (data, labels) in batch.items():\n",
    "#             print(worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_FL_training(model_hyperparams, \n",
    "                        model_structure,\n",
    "                        datasets, \n",
    "                        workers, \n",
    "                        crypto_provider,\n",
    "                        optimizer=th.optim.SGD):\n",
    "    \"\"\" \n",
    "    Simulates a PySyft federated learning cycle using PyTorch, in order\n",
    "    to prove that it can be done conceptually using the PyTorch interface\n",
    "        \n",
    "    Args:\n",
    "        model_hyperparams (model_hyperparams): \n",
    "                                            Parameters defining current experiment\n",
    "        datasets  (sy.FederatedDataLoader): \n",
    "                                        Distributed training datasets\n",
    "        workers   (list(sy.VirtualWorker)): \n",
    "                                        Workers involved in training\n",
    "        crypto_provider (sy.VirtualWorker): \n",
    "                                        Arbiter supervising training\n",
    "        model     (nn.Module): \n",
    "                            Current PyTorch model to train\n",
    "        optimizer (th.optim): \n",
    "                            Optimizer to use\n",
    "    Returns:\n",
    "        global_model (nn.Module) : The trained model \n",
    "        global_states (dict)\n",
    "                        {timestep: nn.Module}\n",
    "                        : The record of trained global models at each timestep.\n",
    "        client_states (dict)\n",
    "                        {timestep {worker_id: nn.Module}}\n",
    "                        : The record of trained models for each worker at each\n",
    "                          timestep. \n",
    "        scale_coeffs (dict)\n",
    "                        {worker_id: float}\n",
    "                        : A dictionary of the update weightings for each worker\n",
    "                          based on individual dataset size. \n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = model_hyperparams['criterion']\n",
    "\n",
    "    def perform_parallel_training(datasets, \n",
    "                                  models, \n",
    "                                  optimizers, \n",
    "                                  criterions, \n",
    "                                  epochs):\n",
    "        \"\"\" \n",
    "        Parallelizes training across each distributed dataset (i.e. simulated worker)\n",
    "        Parallelization here refers to the training of all distributed models per\n",
    "        epoch.\n",
    "        NOTE: Current approach does not have early stopping implemented\n",
    "            \n",
    "        Args:\n",
    "            datasets   (dict(th.utils.data.DataLoader)): \n",
    "                                                       Distributed training datasets\n",
    "            models     (list(nn.Module)): \n",
    "                                        Simulated local models (after distribution)\n",
    "            optimizers (list(th.optim)): \n",
    "                                       Simulated local optimizers (after distribution)\n",
    "            criterions (list(th.nn)):  \n",
    "                                    Simulated local objective function (after distribution)\n",
    "            epochs (int): \n",
    "                        No. of epochs to train each local model\n",
    "        Returns:\n",
    "            trained local models\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            for worker in datasets:\n",
    "#                 print(\"========================\")\n",
    "#                 print(worker)\n",
    "                for batch_idx, batch in enumerate(datasets[worker]):\n",
    "#                     print(batch_idx)\n",
    "                    data = batch[0]\n",
    "                    labels = batch[1]\n",
    "                    '''\n",
    "                    ========================\n",
    "                    Each worker trains its own model individually.\n",
    "                    ========================\n",
    "                    '''\n",
    "                    curr_model = models[worker]\n",
    "                    curr_optimizer = optimizers[worker]\n",
    "                    curr_criterion = criterions[worker]\n",
    "\n",
    "                    # Zero gradients to prevent accumulation                    \n",
    "                    curr_model.train()\n",
    "                    curr_optimizer.zero_grad()\n",
    "\n",
    "                    # Forward Propagation\n",
    "                    predictions = curr_model(data.float())\n",
    "#                     print(predictions.shape)\n",
    "#                     print(labels.shape)\n",
    "\n",
    "                    if model_hyperparams['is_condensed']:\n",
    "                        loss = curr_criterion(predictions, labels.float())\n",
    "                    else:\n",
    "                        loss = curr_criterion(predictions, labels.long())\n",
    "\n",
    "                    # Backward propagation\n",
    "                    loss.backward()\n",
    "                    curr_optimizer.step()\n",
    "\n",
    "                    # Update models, optimisers & losses\n",
    "                    models[worker] = curr_model\n",
    "                    optimizers[worker] = curr_optimizer\n",
    "                    criterions[worker] = curr_criterion\n",
    "\n",
    "                    assert (models[worker] == curr_model and \n",
    "                            optimizers[worker] == curr_optimizer and \n",
    "                            criterions[worker] == curr_criterion)\n",
    "\n",
    "        trained_models = {w: m.send(crypto_provider) for w,m in models.items()}\n",
    "\n",
    "        return trained_models\n",
    "    \n",
    "    def calculate_global_params(global_model, models, datasets):\n",
    "        \"\"\" Aggregates weights from trained locally trained models after a round.\n",
    "        \n",
    "        Args:\n",
    "            global_model   (nn.Module): Global model to be trained federatedly\n",
    "            models   (dict(nn.Module)): Simulated local models (after distribution)\n",
    "            datasets (dict(th.utils.data.DataLoader)): Distributed training datasets\n",
    "        Returns:\n",
    "            Aggregated parameters (OrderedDict)\n",
    "        \"\"\"\n",
    "        param_types = global_model.state_dict().keys()\n",
    "        model_states = {w: m.state_dict() for w,m in models.items()}\n",
    "\n",
    "        \n",
    "        # Find size of all distributed datasets for calculating scaling factor\n",
    "#         print(\"==================\")\n",
    "#         print(datasets)\n",
    "#         obs_counts = {}\n",
    "#         for worker in datasets:\n",
    "#             for batch_idx, batch in enumerate(datasets[worker_id]):\n",
    "#                 data = batch[0]\n",
    "#                 labels = data[1]\n",
    "#                 curr_count = data.shape[0]\n",
    "# #                 print(\"+++++\")\n",
    "# #                 print(curr_count)\n",
    "#                 if worker in obs_counts.keys():\n",
    "#                     obs_counts[worker] += curr_count\n",
    "#                 else:\n",
    "#                     obs_counts[worker] = curr_count\n",
    "        \n",
    "        # Calculate scaling factors for each worker\n",
    "        scale_coeffs = {w: 1/len(list(datasets.keys())) for w in list(datasets.keys())}\n",
    "\n",
    "        # PyTorch models can only swap weights of the same structure\n",
    "        # Hence, aggregate weights while maintaining original layering structure\n",
    "        aggregated_params = OrderedDict()\n",
    "        \n",
    "        '''\n",
    "        ======================\n",
    "        Grab the param_states\n",
    "        ======================\n",
    "        '''\n",
    "        params = {}\n",
    "        \n",
    "        for p_type in param_types:\n",
    "            #param_states = [th.mul(ms[p_type], sc) \n",
    "            #                for ms,sc in zip(model_states, scale_coeffs)]\n",
    "            param_states = [\n",
    "                th.mul(\n",
    "                    model_states[w][p_type],\n",
    "                    scale_coeffs[w]\n",
    "                ).get().get() for w in workers\n",
    "            ]\n",
    "            \n",
    "            '''\n",
    "            ======================\n",
    "            Grab the param_states\n",
    "            ======================\n",
    "            '''   \n",
    "            params.update({p_type : param_states})\n",
    "            \n",
    "            layer_shape = tuple(global_model.state_dict()[p_type].shape)\n",
    "            \n",
    "            '''\n",
    "            ======================\n",
    "            Modification made here to allow multiple layers.\n",
    "            ======================\n",
    "            '''  \n",
    "            aggregated_params[p_type] = th.zeros(param_states[0].shape, dtype=th.float64)\n",
    "            for param_state in param_states:\n",
    "                aggregated_params[p_type] += param_state\n",
    "            aggregated_params[p_type] = aggregated_params[p_type].view(*layer_shape)\n",
    "\n",
    "        return aggregated_params, params, scale_coeffs\n",
    "\n",
    "    # Generate a global model & send it to the TTP\n",
    "    \n",
    "    template_model = Model(model_structure)\n",
    "    \n",
    "    global_model = copy.deepcopy(template_model).send(crypto_provider)\n",
    "    \n",
    "    print(\"Global model parameters:\\n\", [p.location for p in list(global_model.parameters())],\n",
    "          \"\\nID:\\n\", [p.id_at_location for p in list(global_model.parameters())],\n",
    "          \"\\n Cloning effect on global model:\\n\", [p.clone() for p in list(global_model.parameters())])\n",
    "    \n",
    "    rounds = 0\n",
    "    pbar = tqdm(total=model_hyperparams['rounds'], desc='Rounds', leave=True)\n",
    "    \n",
    "    '''\n",
    "    * Dicts for model and client states\n",
    "    \n",
    "    '''\n",
    "    global_states = {}\n",
    "    client_states = {}\n",
    "    global_model_state_dicts = {}\n",
    "\n",
    "    client_template = copy.deepcopy(template_model)\n",
    "    \n",
    "    while rounds < model_hyperparams['rounds']:\n",
    "\n",
    "        local_models = {w: copy.deepcopy(client_template).send(w) for w in workers}\n",
    "\n",
    "        optimizers = {\n",
    "            w: optimizer(\n",
    "                params=model.parameters(), \n",
    "                lr=model_hyperparams['lr'], \n",
    "                weight_decay=model_hyperparams['decay']\n",
    "            ) for w, model in local_models.items()\n",
    "        }\n",
    "        \n",
    "        criterions = {w: criterion(reduction='mean') \n",
    "                      for w,m in local_models.items()}\n",
    "\n",
    "        trained_models = perform_parallel_training(\n",
    "            datasets, \n",
    "            local_models, \n",
    "            optimizers, \n",
    "            criterions, \n",
    "            model_hyperparams['epochs']\n",
    "        )\n",
    "        \n",
    "        aggregated_params, params, scale_coeffs = calculate_global_params(\n",
    "            global_model, \n",
    "            trained_models, \n",
    "            datasets\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        ============================\n",
    "        * Save states to dictionary\n",
    "\n",
    "        '''\n",
    "        global_model_transfer_out = global_model.get()\n",
    "        global_states.update({rounds : copy.deepcopy(global_model_transfer_out)})\n",
    "        global_model_state_dicts.update({rounds : global_model_transfer_out.state_dict()})\n",
    "            \n",
    "        client_states.update({rounds + 1 : params})\n",
    "\n",
    "        # Update weights with aggregated parameters \n",
    "        global_model_transfer_out.load_state_dict(aggregated_params)\n",
    "#         model = copy.deepcopy(global_model_transfer_out)\n",
    "        client_template = copy.deepcopy(global_model_transfer_out)\n",
    "        global_model = global_model_transfer_out.send(crypto_provider)\n",
    "        \n",
    "        rounds += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "    '''\n",
    "    ============================\n",
    "    * Save final global state\n",
    "\n",
    "    '''\n",
    "    global_model_transfer_out = global_model.get()\n",
    "    global_states.update({rounds : copy.deepcopy(global_model_transfer_out)})\n",
    "    global_model = global_model_transfer_out.send(crypto_provider)\n",
    "    global_model_state_dicts.update({rounds : global_model_transfer_out.state_dict()})\n",
    "    pbar.close()\n",
    "\n",
    "    return global_model, global_states, client_states, scale_coeffs, global_model_state_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Default datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to sourcefiles simulating Horizontal learning\n",
    "DATA_IID_1_PATH = Path(\"./unified_dataset_iid_1_1000.csv\").resolve()\n",
    "DATA_IID_2_PATH = Path(\"./unified_dataset_iid_2_1000.csv\").resolve()\n",
    "DATA_nIID_1_PATH = Path(\"./unified_dataset_noniid_1_1000.csv\").resolve()\n",
    "DATA_nIID_2_PATH = Path(\"./unified_dataset_noniid_2_1000.csv\").resolve()\n",
    "DATA_VALIDATION_PATH = Path(\"./unified_dataset_validation_1000.csv\").resolve()\n",
    "DATA_HEADERS = ['age', 'sex', \n",
    "                'cp', 'trestbps', \n",
    "                'chol', 'fbs', \n",
    "                'restecg', 'thalach', \n",
    "                'exang', 'oldpeak', \n",
    "                'slope', 'ca', \n",
    "                'thal', 'target']\n",
    "DATA_SCHEMA = {'age': 'int32',\n",
    "               'sex': 'category', \n",
    "               'cp': 'category', \n",
    "               'trestbps': 'int32', \n",
    "               'chol': 'int32', \n",
    "               'fbs': 'category', \n",
    "               'restecg': 'category', \n",
    "               'thalach': 'int32', \n",
    "               'exang': 'category', \n",
    "               'oldpeak': 'float64', \n",
    "               'slope': 'category', \n",
    "               'ca': 'category', \n",
    "               'thal': 'category', \n",
    "               'target': 'category'}\n",
    "DATA_IDEAL_FEATURES = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n",
    "                       'sex_1.0', 'cp_2', 'cp_3', 'cp_4', 'fbs_1', \n",
    "                       'restecg_1', 'restecg_2', 'exang_1', 'slope_2', 'slope_3',\n",
    "                       'ca_1', 'ca_2', 'ca_3', 'thal_6', 'thal_7']\n",
    "\n",
    "\n",
    "def initialise_datasets(train_src_paths, test_src_path,\n",
    "                        schema=DATA_SCHEMA, is_condensed=True):\n",
    "    \"\"\" Loads in all training & testing datasets, and automates the process\n",
    "        of splitting them into X & y pairs\n",
    "        \n",
    "    Args:\n",
    "        src_paths (list(str)): Paths to training sources\n",
    "        schema         (dict): Universal set of datatypes for source features\n",
    "    Returns:\n",
    "        training_datasets           (dict(tuple(th.Tensor)))\n",
    "        combined_validation_dataset (dict(tuple(th.Tensor)))\n",
    "        testing_dataset             (tuple(th.Tensor))\n",
    "    \"\"\"\n",
    "\n",
    "    def initialise_dataset(data_path):\n",
    "        \"\"\" Loads in data into a preprocessor to obtain symmetrical OHE samples\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path where dataset is stored\n",
    "        Returns:\n",
    "            X_train (th.Tensor)\n",
    "            X_test  (th.Tensor)\n",
    "            y_train (th.Tensor)\n",
    "            y_test  (th.Tensor)\n",
    "        \"\"\"\n",
    "\n",
    "        # Data-specific operation - Ensure that categorical classes are valid\n",
    "         # (This operation is unique to dataset, subjected to changes)\n",
    "        data = pd.read_csv(data_path).replace(\n",
    "            [-9.0, '?'], np.nan\n",
    "        ).replace(\n",
    "            {'ca': 9, 'slope': 0, 'thal': [1,2,5]}, np.nan\n",
    "        )\n",
    "\n",
    "        preprocessor = Preprocessor(data, schema)\n",
    "        preprocessor.interpolate()\n",
    "        X_train, X_test, y_train, y_test = preprocessor.transform(\n",
    "            condense=is_condensed\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            th.from_numpy(X_train), \n",
    "            th.from_numpy(X_test),\n",
    "            th.from_numpy(y_train),\n",
    "            th.from_numpy(y_test)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    training_datasets = {}\n",
    "    validation_datasets = {}   \n",
    "    for s_idx in range(len(train_src_paths)):\n",
    "\n",
    "        curr_path = train_src_paths[s_idx]\n",
    "        X_train, X_test, y_train, y_test = initialise_dataset(curr_path)\n",
    "        \n",
    "        training_datasets[s_idx] = (X_train, y_train)\n",
    "        validation_datasets[s_idx] = (X_test, y_test)\n",
    "     \n",
    "    OHE_testing_datasets = initialise_dataset(test_src_path)\n",
    "    test_X_vals = np.concatenate(OHE_testing_datasets[:2])\n",
    "    test_y_vals = np.concatenate(OHE_testing_datasets[2:])\n",
    "    testing_dataset = (th.from_numpy(test_X_vals), th.from_numpy(test_y_vals))\n",
    "        \n",
    "    return training_datasets, validation_datasets, testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_datasets, \n",
    " validation_datasets, \n",
    " testing_dataset\n",
    ") = initialise_datasets([DATA_nIID_1_PATH, DATA_nIID_2_PATH], DATA_VALIDATION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant A: Synthetic, Even Amounts, Equal DGP, Gaussian Noise\n",
    "### Contributions of clients using this dataset should be equal or close to equal.\n",
    "This function produces a dataset which is evenly distributed among clients, with variable noise. This represents the base test case for CC functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FL functions in this notebook convert individual CSVs for each client into a dictionary of tuples, one tuple for each client, where first entry is a torch tensor of data, and second entry is a torch tensor of labels. in set_up_fl_env, indexing data dictionaries is how datasets are disseminated to workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_A(datagen_config,\n",
    "            is_binary,\n",
    "            num_workers, \n",
    "             val_proportion, \n",
    "             test_proportion,\n",
    "             garbage_proportion,\n",
    "             garbage_severity):\n",
    "    \n",
    "    \"\"\" Takes in a synthetic dataset generated via Skl.datasets, \n",
    "        splits it into num_workers even segments, converts them into \n",
    "        torch tensors, formats into a dictionary, for use in FL \n",
    "        training. \n",
    "        \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to split datasets among. \n",
    "        synth_data (tuple(numpy.ndarray, numpy.ndarray)): A synthetic classification dataset - First entry is data, second is labels. \n",
    "        val_proportion (float): Proportion of datapoints to be used for validation. Float between 0 and 1.\n",
    "        test_proportion (float): Proportion of datapoints to be used for testing. Float between 0 and 1. \n",
    "    Returns:\n",
    "        training_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        validation_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        testing_dataset (tuple(torch.Tensor, torch.Tensor)): A tuple containing test data.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(datagen_config)\n",
    "    \n",
    "    synth_data = skld.make_classification(**datagen_config)\n",
    "    \n",
    "    if is_binary:\n",
    "        synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "    \n",
    "    #============\n",
    "    # Number of data points in synthetic data set\n",
    "    synth_data_length = synth_data[0].shape[0]\n",
    "    \n",
    "    #============\n",
    "    # Number of data points per worker\n",
    "    segment_size = math.floor(synth_data_length / num_workers)\n",
    "    \n",
    "    #============\n",
    "    # Initialize dictionaries for holding datasets.\n",
    "    # defaultdict() used to simplify dict modification.\n",
    "    training_datasets = defaultdict()\n",
    "    validation_datasets = defaultdict()\n",
    "    testing_dataset = None\n",
    "    \n",
    "    #============\n",
    "    # Select the workers whose data will be corrupted. \n",
    "    garbage_indices = np.random.choice(num_workers, int(np.floor(num_workers * garbage_proportion)), replace=False)\n",
    "    \n",
    "    for worker_idx in range(num_workers):\n",
    "        \n",
    "        #============\n",
    "        # Number of data points for test, val, train.\n",
    "        test_size = math.floor(test_proportion * segment_size)\n",
    "        val_size = math.floor(val_proportion * segment_size)\n",
    "        train_size = math.floor((1 - val_proportion - test_proportion) * segment_size)\n",
    "        \n",
    "#         print(test_size, val_size, train_size)\n",
    "        \n",
    "        #============\n",
    "        # Start and end index of worker's datasegment out of entire dataset. \n",
    "        start_idx = worker_idx * segment_size\n",
    "        end_idx = (worker_idx + 1) * segment_size\n",
    "        \n",
    "#         print(start_idx, end_idx)\n",
    "        \n",
    "        #============\n",
    "        # Within each segment, the indexes of test, val, train subsegments.\n",
    "        test_idx = start_idx + test_size\n",
    "        val_idx = test_idx + val_size\n",
    "        train_idx = val_idx + train_size\n",
    "        \n",
    "#         print(test_idx, val_idx, train_idx)\n",
    "\n",
    "        #============\n",
    "        # Slice dataset by above segments. Convert to torch tensors. \n",
    "        validation_datasets[worker_idx] = (th.from_numpy(synth_data[0][test_idx:val_idx, :]), \n",
    "                                           th.from_numpy(synth_data[1][test_idx:val_idx]))\n",
    "        \n",
    "        training_datasets[worker_idx] = (th.from_numpy(synth_data[0][val_idx:train_idx, :]), \n",
    "                                         th.from_numpy(synth_data[1][val_idx:train_idx]))\n",
    "        \n",
    "        #============\n",
    "        # Scatter random noise over data matrices of selected workers.\n",
    "        if worker_idx in garbage_indices:\n",
    "            \n",
    "            val_shape = validation_datasets[worker_idx][0].shape\n",
    "            train_shape = training_datasets[worker_idx][0].shape\n",
    "            \n",
    "            val_noise = np.random.rand(val_shape[0], val_shape[1]) * garbage_severity\n",
    "            train_noise = np.random.rand(train_shape[0], train_shape[1]) * garbage_severity\n",
    "            \n",
    "            validation_datasets[worker_idx] = (validation_datasets[worker_idx][0] + th.from_numpy(val_noise), \n",
    "                                               validation_datasets[worker_idx][1])  \n",
    "            training_datasets[worker_idx] = (training_datasets[worker_idx][0] + th.from_numpy(train_noise), \n",
    "                                             training_datasets[worker_idx][1])\n",
    "\n",
    "        \n",
    "        #============\n",
    "        # Testing dataset is in tuple form since it is used by ttp.\n",
    "        # Each segment contributes a portion of the total, hence the\n",
    "        # concatenation. \n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (synth_data[0][start_idx:test_idx, :], \n",
    "                                synth_data[1][start_idx:test_idx])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                synth_data[0][start_idx:test_idx, :]), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                synth_data[1][start_idx:test_idx]), \n",
    "                                               axis = 0))\n",
    "    \n",
    "    #============\n",
    "    # Convert testing dataset to torch tensors. \n",
    "    testing_dataset = (th.from_numpy(testing_dataset[0]), \n",
    "                        th.from_numpy(testing_dataset[1]))\n",
    "        \n",
    "    returned_info_dict = {'garbage_indices': garbage_indices, 'worker_proportions': []}\n",
    "    return training_datasets, validation_datasets, testing_dataset, returned_info_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant B: Synthetic, Uneven Amounts, Equal DGP, No Additional Noise. \n",
    "### Contributions of clients using this dataset should be based on observation numbers per client. \n",
    "This function produces a dataset which is unevenly distributed among clients, with variable noise, but from the same distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_B(datagen_config,\n",
    "            is_binary,\n",
    "            num_workers, \n",
    "            val_proportion, \n",
    "            test_proportion,\n",
    "            garbage_proportion,\n",
    "            garbage_severity):\n",
    "    \n",
    "    \"\"\" Takes in a synthetic dataset generated via Skl.datasets, \n",
    "        splits it into num_workers even segments, converts them into \n",
    "        torch tensors, formats into a dictionary, for use in FL \n",
    "        training. \n",
    "        \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to split datasets among. \n",
    "        synth_data (tuple(numpy.ndarray, numpy.ndarray)): A synthetic classification dataset - First entry is data, second is labels. \n",
    "        val_proportion (float): Proportion of datapoints to be used for validation. Float between 0 and 1.\n",
    "        test_proportion (float): Proportion of datapoints to be used for testing. Float between 0 and 1. \n",
    "    Returns:\n",
    "        training_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        validation_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        testing_dataset (tuple(torch.Tensor, torch.Tensor)): A tuple containing test data.\n",
    "    \"\"\"\n",
    "    returned_info_dict = {}\n",
    "    \n",
    "    synth_data = skld.make_classification(**datagen_config)\n",
    "    \n",
    "    if is_binary:\n",
    "        synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "    \n",
    "    worker_proportions = [np.random.random() for i in range(num_workers)]\n",
    "    worker_proportions /= np.sum(worker_proportions)\n",
    "    \n",
    "#     print(worker_proportions)\n",
    "    \n",
    "    #============\n",
    "    # Number of data points in synthetic data set\n",
    "    synth_data_length = synth_data[0].shape[0]\n",
    "    \n",
    "    #============\n",
    "    # Initialize dictionaries for holding datasets.\n",
    "    # defaultdict() used to simplify dict modification.\n",
    "    training_datasets = defaultdict()\n",
    "    validation_datasets = defaultdict()\n",
    "    testing_dataset = None\n",
    "\n",
    "    previous_end_idx = 0\n",
    "    for worker_idx in range(num_workers):\n",
    "        \n",
    "        #============\n",
    "        # Number of data points for test, val, train.\n",
    "        test_size = math.floor((test_proportion * worker_proportions[worker_idx]) * synth_data_length)\n",
    "        val_size = math.floor((val_proportion * worker_proportions[worker_idx]) * synth_data_length)\n",
    "        train_size = math.floor(((1 - val_proportion - test_proportion) * worker_proportions[worker_idx]) * synth_data_length)\n",
    "         \n",
    "#         print(test_size, val_size, train_size)\n",
    "        \n",
    "        #============\n",
    "        # Start and end index of worker's datasegment out of entire dataset. \n",
    "        \n",
    "#         print(worker_proportions[worker_idx] * synth_data_length)\n",
    "        start_idx = previous_end_idx\n",
    "        end_idx = math.floor(previous_end_idx + (worker_proportions[worker_idx] * synth_data_length))\n",
    "        previous_end_idx = end_idx\n",
    "#         print(start_idx, end_idx)\n",
    "        \n",
    "        #============\n",
    "        # Within each segment, the indexes of test, val, train subsegments.\n",
    "        test_idx = start_idx + test_size\n",
    "        val_idx = test_idx + val_size\n",
    "        train_idx = val_idx + train_size\n",
    "        \n",
    "#         print(test_idx, val_idx, train_idx)\n",
    "\n",
    "        #============\n",
    "        # Slice dataset by above segments. Convert to torch tensors. \n",
    "        validation_datasets[worker_idx] = (th.from_numpy(synth_data[0][test_idx:val_idx, :]), \n",
    "                                           th.from_numpy(synth_data[1][test_idx:val_idx]))\n",
    "        \n",
    "        training_datasets[worker_idx] = (th.from_numpy(synth_data[0][val_idx:train_idx, :]), \n",
    "                                         th.from_numpy(synth_data[1][val_idx:train_idx]))\n",
    "        \n",
    "        #============\n",
    "        # Testing dataset is in tuple form since it is used by ttp.\n",
    "        # Each segment contributes a portion of the total, hence the\n",
    "        # concatenation. \n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (synth_data[0][start_idx:test_idx, :], \n",
    "                                synth_data[1][start_idx:test_idx])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                synth_data[0][start_idx:test_idx, :]), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                synth_data[1][start_idx:test_idx]), \n",
    "                                               axis = 0))\n",
    "    \n",
    "    #============\n",
    "    # Convert testing dataset to torch tensors. \n",
    "    testing_dataset = (th.from_numpy(testing_dataset[0]), \n",
    "                        th.from_numpy(testing_dataset[1]))\n",
    "        \n",
    "    returned_info_dict = {'garbage_indices': [], 'worker_proportions': worker_proportions}\n",
    "    return training_datasets, validation_datasets, testing_dataset, returned_info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 0.9225398153900244\n",
      "y: 0.4007348708975329\n",
      "[0.9225398153900244, 1.3232746862875573]\n"
     ]
    }
   ],
   "source": [
    "proportions = [x]\n",
    "x = np.random.random()\n",
    "print(\"x: {x}\".format(x = x))\n",
    "y = np.random.normal(1, scale=0.4, size = None)\n",
    "print(\"y: {y}\".format(y = y))\n",
    "proportions = [x, (x + y)]\n",
    "print(proportions)\n",
    "# print(sum(proportions))\n",
    "# # print(min(proportions))\n",
    "# proportions = [x / sum(proportions), ((x + y)) / sum(proportions)]\n",
    "# print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0.4, scale=0.001, size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96234434 1.12841068 1.05760708 0.71440494]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for i in range(4):\n",
    "    test.append(np.random.normal(1, scale=0.4, size = None))\n",
    "test = (np.array(test))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13626521458157845\n"
     ]
    }
   ],
   "source": [
    "worker_proportions = test\n",
    "average_difference = 0\n",
    "for i in worker_proportions:\n",
    "    avg_diff_per_worker = 0\n",
    "    for j in worker_proportions:\n",
    "        if i != j:\n",
    "            avg_diff_per_worker += abs(i - j)\n",
    "#     print(avg_diff_per_worker)\n",
    "            \n",
    "    average_difference += (avg_diff_per_worker / (len(worker_proportions) - 1))\n",
    "    \n",
    "print(average_difference / len(worker_proportions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22470950122727237, 0.23975070923995856, 0.2569738538954861, 0.012919479225867737, 0.13895059447922115, 0.14234720883946675, 0.22506705432002155, 0.0021860759137620977, 0.33720510852075863, 0.3163645883628665, 0.21143379555246772, 0.11231988759584062, 0.22466781026175175, 0.030445576279362244, 0.38600324618341575, 0.39606567413674076, 0.16461733094911307, 0.34105543820780226, 0.0790866246326059, 0.3432027501822075, 0.33760505875516, 0.26186193378499373, 0.22565345469568282, 0.007054403377193453, 0.10118440256808321, 0.03092697388128296, 0.04534309203436724, 0.30210542116887973, 0.3323023630425529, 0.10466432373850819]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for i in range(30):\n",
    "    test.append(np.random.random() * 0.4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36906296, 0.45056731, 0.16122364, 0.01914609])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_proportions = [np.random.random() for i in range(4)]\n",
    "worker_proportions /= np.sum(worker_proportions)\n",
    "worker_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [0.4, 0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "worker_proportions = proportions\n",
    "average_difference = 0\n",
    "for i in worker_proportions:\n",
    "    avg_diff_per_worker = 0\n",
    "    for j in worker_proportions:\n",
    "        if i != j:\n",
    "            avg_diff_per_worker += abs(i - j)\n",
    "#     print(avg_diff_per_worker)\n",
    "    average_difference += (avg_diff_per_worker)\n",
    "print(average_difference / len(worker_proportions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proportions = [[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                   [0.3, 0.3, 0.13, 0.13, 0.13], \n",
    "                   [0.4, 0.3, 0.2, 0.05, 0.05],\n",
    "                   [0.7, 0.2, 0.03, 0.03, 0.03]]\n",
    "\n",
    "DGPs = [[(0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1)],\n",
    "       [(0, 0.2), (0, 0.2), (0, 0.1), (0, 0.1), (0, 0.1)], \n",
    "       [(0, 0.3), (0, 0.2), (0, 0.1), (0, 0.1), (0, 0.1)],\n",
    "       [(0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1), (0, 0.1)]]\n",
    "\n",
    "distribution_of_labels = [[(0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)],\n",
    "                           [(0.7, 0.3), (0.6, 0.4), (0.6, 0.4), (0.5, 0.5), (0.5, 0.5)], \n",
    "                           [(0.85, 0.15), (0.8, 0.2), (0.7, 0.3), (0.6, 0.4), (0.5, 0.5)],\n",
    "                           [(1, 0), (0, 1), (1, 0), (0, 1), (0, 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant C: Synthetic, Even Amounts, Unequal DGPS, No Additional Noise, Test dataset from the mean DGP. \n",
    "### Contributions of clients using this dataset should be based on observation numbers per client. \n",
    "This function produces a dataset which is unevenly distributed among clients, with variable noise, but from the same distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant D: Synthetic, Uneven Amounts, Unequal DGPS, No Additional Noise, Test dataset from the DGP whose client has the least observations. \n",
    "### Contributions of clients using this dataset should be based on observation numbers per client. \n",
    "This function produces a dataset which is unevenly distributed among clients, with variable noise, but from the same distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_C(datagen_config,\n",
    "            is_binary,\n",
    "            num_workers, \n",
    "             val_proportion, \n",
    "             test_proportion,\n",
    "             garbage_proportion,\n",
    "             garbage_severity,\n",
    "            average_quantity_difference = 0.4):\n",
    "    \n",
    "    \"\"\" Takes in a synthetic dataset generated via Skl.datasets, \n",
    "        splits it into num_workers even segments, converts them into \n",
    "        torch tensors, formats into a dictionary, for use in FL \n",
    "        training. \n",
    "        \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to split datasets among. \n",
    "        synth_data (tuple(numpy.ndarray, numpy.ndarray)): A synthetic classification dataset - First entry is data, second is labels. \n",
    "        val_proportion (float): Proportion of datapoints to be used for validation. Float between 0 and 1.\n",
    "        test_proportion (float): Proportion of datapoints to be used for testing. Float between 0 and 1. \n",
    "    Returns:\n",
    "        training_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        validation_datasets (defaultdict{\n",
    "                                    int:tuple(torch.Tensor, torch.Tensor)\n",
    "                                    }) : A dictionary where data is organized as tuple of torch tensors and indexed by the worker\n",
    "                                        it belongs to. Follows same ordering as synth_data.\n",
    "                                        \n",
    "        testing_dataset (tuple(torch.Tensor, torch.Tensor)): A tuple containing test data.\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(datagen_config)\n",
    "    \n",
    "    synth_data = skld.make_classification(**datagen_config)\n",
    "    \n",
    "    if is_binary:\n",
    "        synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "    \n",
    "    #============\n",
    "    # Number of data points in synthetic data set\n",
    "    synth_data_length = synth_data[0].shape[0]\n",
    "    \n",
    "    #============\n",
    "    # Number of data points per worker\n",
    "    segment_size = math.floor(synth_data_length / num_workers)\n",
    "    \n",
    "    #============\n",
    "    # Initialize dictionaries for holding datasets.\n",
    "    # defaultdict() used to simplify dict modification.\n",
    "    training_datasets = defaultdict()\n",
    "    validation_datasets = defaultdict()\n",
    "    testing_dataset = None\n",
    "    \n",
    "    #============\n",
    "    # Select the workers whose data will be corrupted. \n",
    "    garbage_indices = np.random.choice(num_workers, int(np.floor(num_workers * garbage_proportion)), replace=False)\n",
    "    \n",
    "    for worker_idx in range(num_workers):\n",
    "        \n",
    "        #============\n",
    "        # Number of data points for test, val, train.\n",
    "        test_size = math.floor(test_proportion * segment_size)\n",
    "        val_size = math.floor(val_proportion * segment_size)\n",
    "        train_size = math.floor((1 - val_proportion - test_proportion) * segment_size)\n",
    "        \n",
    "#         print(test_size, val_size, train_size)\n",
    "        \n",
    "        #============\n",
    "        # Start and end index of worker's datasegment out of entire dataset. \n",
    "        start_idx = worker_idx * segment_size\n",
    "        end_idx = (worker_idx + 1) * segment_size\n",
    "        \n",
    "#         print(start_idx, end_idx)\n",
    "        \n",
    "        #============\n",
    "        # Within each segment, the indexes of test, val, train subsegments.\n",
    "        test_idx = start_idx + test_size\n",
    "        val_idx = test_idx + val_size\n",
    "        train_idx = val_idx + train_size\n",
    "        \n",
    "#         print(test_idx, val_idx, train_idx)\n",
    "\n",
    "        #============\n",
    "        # Slice dataset by above segments. Convert to torch tensors. \n",
    "        validation_datasets[worker_idx] = (th.from_numpy(synth_data[0][test_idx:val_idx, :]), \n",
    "                                           th.from_numpy(synth_data[1][test_idx:val_idx]))\n",
    "        \n",
    "        training_datasets[worker_idx] = (th.from_numpy(synth_data[0][val_idx:train_idx, :]), \n",
    "                                         th.from_numpy(synth_data[1][val_idx:train_idx]))\n",
    "        \n",
    "        #============\n",
    "        # Scatter random noise over data matrices of selected workers.\n",
    "        if worker_idx in garbage_indices:\n",
    "            \n",
    "            val_shape = validation_datasets[worker_idx][0].shape\n",
    "            train_shape = training_datasets[worker_idx][0].shape\n",
    "            \n",
    "            val_noise = np.random.rand(val_shape[0], val_shape[1]) * garbage_severity\n",
    "            train_noise = np.random.rand(train_shape[0], train_shape[1]) * garbage_severity\n",
    "            \n",
    "            validation_datasets[worker_idx] = (validation_datasets[worker_idx][0] + th.from_numpy(val_noise), \n",
    "                                               validation_datasets[worker_idx][1])  \n",
    "            training_datasets[worker_idx] = (training_datasets[worker_idx][0] + th.from_numpy(train_noise), \n",
    "                                             training_datasets[worker_idx][1])\n",
    "\n",
    "        \n",
    "        #============\n",
    "        # Testing dataset is in tuple form since it is used by ttp.\n",
    "        # Each segment contributes a portion of the total, hence the\n",
    "        # concatenation. \n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (synth_data[0][start_idx:test_idx, :], \n",
    "                                synth_data[1][start_idx:test_idx])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                synth_data[0][start_idx:test_idx, :]), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                synth_data[1][start_idx:test_idx]), \n",
    "                                               axis = 0))\n",
    "    \n",
    "    #============\n",
    "    # Convert testing dataset to torch tensors. \n",
    "    testing_dataset = (th.from_numpy(testing_dataset[0]), \n",
    "                        th.from_numpy(testing_dataset[1]))\n",
    "        \n",
    "    returned_info_dict = {'garbage_indices': garbage_indices, 'worker_proportions': []}\n",
    "    return training_datasets, validation_datasets, testing_dataset, returned_info_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Synthetic Datasets\n",
    "Produce synthetic datasets according to the settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#============\n",
    "# Settings for sklearn.datasets.make_classification function. \n",
    "binary_synth_datagen_config = {\n",
    "    \"n_samples\": 3000, \n",
    "     \"n_features\": 20, \n",
    "     \"n_informative\": 20, \n",
    "     \"n_redundant\": 0, \n",
    "     \"n_repeated\": 0, \n",
    "     \"n_classes\": 2, \n",
    "     \"n_clusters_per_class\": 1, \n",
    "     \"weights\": None, \n",
    "     \"flip_y\": 0.01, \n",
    "     \"class_sep\": 0.1,\n",
    "     \"hypercube\": True, \n",
    "     \"shift\": 0.0, \n",
    "     \"scale\": 1.0, \n",
    "     \"shuffle\": True, \n",
    "     \"random_state\": None\n",
    "}\n",
    "\n",
    "#============\n",
    "# Binary dataset creation and formatting. \n",
    "\n",
    "# synth_data = skld.make_classification(**binary_synth_datagen_config)\n",
    "# synth_data = (synth_data[0], np.reshape(synth_data[1], (-1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "multiclass_synth_datagen_config = {\n",
    "    \"n_samples\": 3000, \n",
    "     \"n_features\": 20, \n",
    "     \"n_informative\": 20, \n",
    "     \"n_redundant\": 0, \n",
    "     \"n_repeated\": 0, \n",
    "     \"n_classes\": 4, \n",
    "     \"n_clusters_per_class\": 1, \n",
    "     \"weights\": None, \n",
    "     \"flip_y\": 0.01, \n",
    "     \"class_sep\": 0.1,\n",
    "     \"hypercube\": False, \n",
    "     \"shift\": 0.0, \n",
    "     \"scale\": 1.0, \n",
    "     \"shuffle\": True, \n",
    "     \"random_state\": None\n",
    "}\n",
    "\n",
    "#============\n",
    "# Multiclass dataset creation. \n",
    "\n",
    "# synth_data = skld.make_classification(**multiclass_synth_datagen_config)\n",
    "\n",
    "#============\n",
    "# Settings for dataset split function in above cell. \n",
    "synth_gen_config = {\n",
    "    \"datagen_config\": binary_synth_datagen_config,\n",
    "    \"is_binary\": True,\n",
    "    \"num_workers\": 5,\n",
    "    \"val_proportion\": 0.2,\n",
    "    \"test_proportion\": 0.1, \n",
    "    \"garbage_proportion\": 0,\n",
    "    \"garbage_severity\": 3\n",
    "}\n",
    "\n",
    "#============\n",
    "# Perform dataset splitting.\n",
    "# (training_datasets, validation_datasets, testing_dataset, returned_info_dict) = synth_A(**synth_gen_config)\n",
    "\n",
    "(training_datasets, validation_datasets, testing_dataset, returned_info_dict) = synth_B(**synth_gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 20])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 20])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14847446, 0.13371493, 0.42542594, 0.01199297, 0.28039171])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_info_dict['worker_proportions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 1])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 20])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datasets[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean and standard deviation of dataset quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_quantity(dataset_dict):\n",
    "    x = []\n",
    "    for client_idx in dataset_dict:\n",
    "        x.append(dataset_dict[client_idx][0].shape[0])\n",
    "    print(np.array(x).mean())\n",
    "    print(np.array(x).std())\n",
    "    print((np.array(x) - np.array(x).mean()) / (np.array(x).std() + 0.0000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.4\n",
      "296.46490517428873\n",
      "[-0.36564193 -0.47020743  1.59749094 -1.33034296  0.56870138]\n"
     ]
    }
   ],
   "source": [
    "measure_quantity(training_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average difference in quantity between clients. For each client, calculate the average difference in quantity between that client and all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_quantity_per_client(dataset_dict):\n",
    "    total = 0\n",
    "    average_difference_per_client = {}\n",
    "    for client_idx in dataset_dict:\n",
    "        total_per_client = 0\n",
    "        client_quantity = dataset_dict[client_idx][0].shape[0]\n",
    "        for other_client_idx in dataset_dict:\n",
    "            if client_idx != other_client_idx:\n",
    "                other_client_quantity = dataset_dict[other_client_idx][0].shape[0]\n",
    "                total_per_client += abs(client_quantity - other_client_quantity)\n",
    "        total_per_client /= (len(dataset_dict.keys()) - 1)\n",
    "        \n",
    "        average_difference_per_client.update({client_idx: total_per_client})\n",
    "        total += total_per_client\n",
    "        \n",
    "    total /= len(dataset_dict.keys())\n",
    "    return total, average_difference_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330.6, {0: 257.0, 1: 411.75, 2: 407.0, 3: 327.75, 4: 249.5})"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_quantity_per_client(training_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(training_datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'num_samples', 'user_data'])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = json.load(open('./train/data_niid_0_keep_5_train_6.json'))\n",
    "test_data = json.load(open('./test/data_niid_0_keep_5_test_6.json'))\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels_per_client(dataset_dict):\n",
    "    client_counts = defaultdict(int)\n",
    "    max_num_labels = 0\n",
    "    for client_idx in dataset_dict:\n",
    "        label_counts = defaultdict(int)\n",
    "        labels = dataset_dict[client_idx][1].view(-1, )\n",
    "        for label in labels.tolist():\n",
    "            label_counts[label] += 1\n",
    "            if len(labels.tolist()) > max_num_labels:\n",
    "                max_num_labels = len(labels.unique())\n",
    "        client_counts[client_idx] = label_counts\n",
    "    return client_counts, max_num_labels\n",
    "\n",
    "def average_class_discrepancy_per_client(dataset_dict):\n",
    "    average_label_difference_per_client = {}\n",
    "    client_counts, max_num_labels = average_difference_label_numbers(training_datasets)\n",
    "    average_percentage_difference_relative_to_individual = 0\n",
    "    for client_idx in client_counts:\n",
    "        total_diff = 0\n",
    "        total = 0\n",
    "        for label in client_counts[client_idx]:\n",
    "            label_count = client_counts[client_idx][label]\n",
    "            total += label_count\n",
    "            for other_label in client_counts[client_idx]:\n",
    "                if label != other_label:\n",
    "                    other_label_count = client_counts[client_idx][other_label]\n",
    "                    total_diff += abs(label_count - other_label_count)\n",
    "        average_label_difference_per_client.update({client_idx: total_diff/total})\n",
    "        total_diff /= len(client_counts[client_idx].keys()) + 0.00001\n",
    "        \n",
    "#         print(round(total_diff))\n",
    "#         print(round(total_diff) / (total + 0.00001))\n",
    "        average_percentage_difference_relative_to_individual += round(total_diff) / (total + 0.00001)\n",
    "    return average_percentage_difference_relative_to_individual / len(client_counts.keys()), average_label_difference_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {0: defaultdict(int, {0: 152, 1: 159}),\n",
       "              1: defaultdict(int, {1: 137, 0: 143}),\n",
       "              2: defaultdict(int, {0: 467, 1: 426}),\n",
       "              3: defaultdict(int, {0: 12, 1: 13}),\n",
       "              4: defaultdict(int, {1: 290, 0: 298})}),\n",
       " 2)"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_labels_per_client(training_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_label_distribution_per_client(training_datasets):\n",
    "    client_counts, num_labels = count_labels_per_client(training_datasets)\n",
    "    count_dict = defaultdict(list)\n",
    "    for i in range(num_labels):\n",
    "        for client_idx in client_counts:\n",
    "            count_dict[i].append(client_counts[client_idx][i])\n",
    "    mean_std_dict = copy.deepcopy(count_dict)\n",
    "    for label in count_dict:\n",
    "        mean = np.array(count_dict[label]).mean()\n",
    "        std = np.array(count_dict[label]).std()\n",
    "        for client_idx in client_counts:\n",
    "            client_counts[client_idx][label] = (client_counts[client_idx][label] - mean) / std\n",
    "#         for i in range(len(count_dict[label])):\n",
    "#             mean_std_dict[i] = (mean_std_dict[i] - mean) / std\n",
    "    return client_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: defaultdict(int,\n",
       "                         {0: -0.4015320976299024, 1: -0.32580714537395156}),\n",
       "             1: defaultdict(int,\n",
       "                         {1: -0.4816279540310588, 0: -0.45944538094190757}),\n",
       "             2: defaultdict(int,\n",
       "                         {0: 1.6254328182902777, 1: 1.565290850600941}),\n",
       "             3: defaultdict(int,\n",
       "                         {0: -1.3024053935944269, 1: -1.3598906937347544}),\n",
       "             4: defaultdict(int,\n",
       "                         {1: 0.6020349425388235, 0: 0.5379500538759588})})"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_label_distribution_per_client(training_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05129574, 0.22208825, 0.1289808 , 0.03305123, 0.15348252,\n",
       "       0.09724967, 0.15352131, 0.01237889, 0.09860318, 0.04934841])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_info_dict['worker_proportions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_samples_per_client(dataset_dict):\n",
    "    x = []\n",
    "    for client_idx in dataset_dict:\n",
    "        x.append(dataset_dict[client_idx][0].shape[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mean_label_discrepancy_per_client(training_datasets):\n",
    "    client_counts, num_labels = count_labels_per_client(training_datasets)\n",
    "    differences = defaultdict(float)\n",
    "    print(client_counts)\n",
    "    discrepancies = []\n",
    "    for client_idx in client_counts:\n",
    "        total = 0\n",
    "        for label in client_counts[client_idx]:\n",
    "            for other_label in client_counts[client_idx]:\n",
    "                if label != other_label:\n",
    "                    total += np.abs(client_counts[client_idx][label] - client_counts[client_idx][other_label])\n",
    "        \n",
    "        discrepancies.append(total / 2)\n",
    "#     discrepancies = (np.array(discrepancies) - np.min(np.array(discrepancies))) / np.max(np.array(discrepancies))\n",
    "    totals = count_samples_per_client(training_datasets)\n",
    "#     print(np.array(discrepancies) / np.array(totals))\n",
    "    discrepancies = np.array(discrepancies) / np.array(totals)\n",
    "    print(discrepancies)\n",
    "#     print((np.array(discrepancies) - np.min(np.array(discrepancies))) / np.max(np.array(discrepancies)))\n",
    "    print(np.array(discrepancies).mean())\n",
    "    print(np.array(discrepancies).std())\n",
    "    print((np.array(discrepancies) - np.array(discrepancies).mean()) / np.array(discrepancies).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: defaultdict(<class 'int'>, {0: 152, 1: 159}), 1: defaultdict(<class 'int'>, {1: 137, 0: 143}), 2: defaultdict(<class 'int'>, {0: 467, 1: 426}), 3: defaultdict(<class 'int'>, {0: 12, 1: 13}), 4: defaultdict(<class 'int'>, {1: 290, 0: 298})})\n",
      "[0.02250804 0.02142857 0.04591265 0.04       0.01360544]\n",
      "0.02869094123320302\n",
      "0.012190363260188545\n",
      "[-0.50719593 -0.59574679  1.41273171  0.92770482 -1.23749381]\n"
     ]
    }
   ],
   "source": [
    "measure_mean_label_discrepancy_per_client(training_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['user_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_classes(df):\n",
    "    num = 0\n",
    "    for user in df['users']:\n",
    "        num_class = len(set(df['user_data'][user]['y']))\n",
    "        if num_class > num:\n",
    "            num = num_class\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_LEAF_train_data(df):\n",
    "    dataset_dict = defaultdict()\n",
    "    num_classes = find_num_classes(df)\n",
    "    \n",
    "    for user in df['users']:\n",
    "        if num_classes > 2:\n",
    "            dataset_dict[int(user)] = (th.tensor(df['user_data'][user]['x']), \n",
    "                                  th.tensor(df['user_data'][user]['y']))\n",
    "        else:\n",
    "            dataset_dict[int(user)] = (th.tensor(df['user_data'][user]['x']), \n",
    "                                    th.tensor(np.reshape(df['user_data'][user]['y'], (-1, 1))))\n",
    "    return dataset_dict\n",
    "\n",
    "def format_LEAF_test_data(df):\n",
    "    testing_dataset = None\n",
    "    for user in df['users']:\n",
    "        if testing_dataset is None:\n",
    "            testing_dataset = (df['user_data'][user]['x'], \n",
    "                                df['user_data'][user]['y'])\n",
    "        else:\n",
    "            testing_dataset = (np.concatenate((testing_dataset[0], \n",
    "                                                df['user_data'][user]['x']), \n",
    "                                               axis = 0), \n",
    "                                np.concatenate((testing_dataset[1], \n",
    "                                                df['user_data'][user]['y']), \n",
    "                                               axis = 0))\n",
    "    testing_dataset = (th.tensor(testing_dataset[0]), \n",
    "                        th.tensor(np.reshape(testing_dataset[1], (-1, 1))))\n",
    "    return testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = format_LEAF_train_data(train_data)\n",
    "validation_datasets = format_LEAF_train_data(test_data)\n",
    "testing_dataset = format_LEAF_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 40])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 1])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
